{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hagaakari/.pyenv/versions/3.8.0/lib/python3.8/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "'''# 89. 事前学習済み言語モデルからの転移学習\n",
    "事前学習済み言語モデル（例えばBERTなど）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．\n",
    "'''\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import tokens2ids\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', \n",
    "                                                      num_labels=4,\n",
    "                                                      output_attentions = False,\n",
    "                                                      output_hidden_states = False,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, optimizer, loader, test_loader, ds_size, device, max_iter=10):\n",
    "    for epoch in range(max_iter):\n",
    "        n_correct = 0\n",
    "        total_loss = 0\n",
    "        for i, (inputs, labels) in enumerate(tqdm(loader)):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            loss, logits = model(inputs, labels=labels)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data\n",
    "            logits = np.argmax(logits.data.numpy(), axis=1)\n",
    "            labels = labels.data.numpy()\n",
    "            for logit, label in zip(logits, labels):\n",
    "                if logit == label:\n",
    "                    n_correct += 1\n",
    "\n",
    "            print('total_loss: %f\\t n_correct: %d' % (total_loss, n_correct))\n",
    "\n",
    "        print('epoch: ', epoch)\n",
    "        print('[train]\\tloss: %f accuracy: %f' % (loss, n_correct/ds_size))\n",
    "\n",
    "        test_loss, test_acc = evaluate(model, test_loader)\n",
    "        print('[test]\\tloss: %f accuracy: %f' % (test_loss, test_acc))\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "class Mydatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = torch.tensor(labels).long()\n",
    "        self.datanum = len(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label = self.labels[idx]\n",
    "        return out_data, out_label\n",
    "\n",
    "\n",
    "\n",
    "def normalize(doc):\n",
    "    if type(doc) is not str:\n",
    "        doc = str(doc)[1:-1]\n",
    "    doc = re.sub(r\"[',.]\", '', doc)   # 記号を削除\n",
    "    doc = re.sub(r\"[\\n\\t]\", ' ', doc)\n",
    "    doc = re.sub(r\"https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+\", '', doc)\n",
    "    doc = re.sub(r\" {2,}\", ' ', doc)  # 2回以上続くスペースを削除\n",
    "    doc = re.sub(r\" *?$\", '', doc)    # 行頭と行末のスペースを削除\n",
    "    doc = re.sub(r\"^ *?\", '', doc)\n",
    "    doc = doc.lower()                 # 小文字に統一\n",
    "    return doc\n",
    "\n",
    "\n",
    "def accuracy(pred, label):\n",
    "    pred = np.argmax(pred.data.numpy(), axis=1)  # 行ごとに最大値のインデックスを取得する．\n",
    "    label = label.data.numpy()\n",
    "    return (pred == label).mean()\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    for inputs, labels, lengs in loader:\n",
    "        outputs = model(inputs, lengs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        acc = accuracy(outputs, labels)\n",
    "    return loss.data, acc\n",
    "\n",
    "\n",
    "def preprocessor(doc):\n",
    "    doc = normalize(doc)\n",
    "    tokens = tokenizer.encode_plus(\n",
    "        doc,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        pad_to_max_length=True,\n",
    "        return_tensors='pt')\n",
    "    tokens = tokens['input_ids']\n",
    "    return tokens.squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dw = 300\n",
    "dh = 50\n",
    "L = 4\n",
    "batch_size = 8\n",
    "columns = ('category', 'title')\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "test = pd.read_csv('../../data/NewsAggregatorDataset/test.txt',\n",
    "                   names=columns, sep='\\t')\n",
    "\n",
    "\n",
    "test['tokens'] = test.title.apply(preprocessor)\n",
    "train['tokens'] = train.title.apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['len'] = train.tokens.apply(len)\n",
    "max_len = train[train.len<512].sort_values('len', ascending=False).len.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs torch.Size([1024, 512]) tensor([[  101, 10651,  1021,  ...,     0,     0,     0],\n",
      "        [  101,  2016,  2015,  ...,     0,     0,     0],\n",
      "        [  101,  3795,  6089,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 18921,  2595,  ...,     0,     0,     0],\n",
      "        [  101,  9944,  6468,  ...,     0,     0,     0],\n",
      "        [  101,  2054,  3071,  ...,     0,     0,     0]])\n",
      "labels size torch.Size([1024]) tensor([1, 2, 0,  ..., 0, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "X_train = train['tokens']\n",
    "X_test = test['tokens']\n",
    "\n",
    "\n",
    "label2int = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "Y_train = train.category.map(label2int)\n",
    "Y_test = test.category.map(label2int)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "trainset = Mydatasets(X_train, Y_train)\n",
    "testset = Mydatasets(X_test, Y_test)\n",
    "loader = DataLoader(trainset, batch_size=batch_size)\n",
    "test_loader = DataLoader(testset, batch_size=testset.__len__())\n",
    "\n",
    "model = model.to(device)\n",
    "ds_size = trainset.__len__()\n",
    "\n",
    "trainer(model, optimizer, loader, test_loader, ds_size, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
