{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# 89. 事前学習済み言語モデルからの転移学習\n",
    "事前学習済み言語モデル（例えばBERTなど）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．\n",
    "'''\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "from utils import tokens2ids\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', \n",
    "                                                      num_labels=4,\n",
    "                                                      output_attentions = False,\n",
    "                                                      output_hidden_states = False,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds.detach().numpy(), axis=1)\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat)\n",
    "\n",
    "\n",
    "def trainer(model, optimizer, loader, test_loader, ds_size, device, max_iter=10):\n",
    "    for epoch in range(max_iter):\n",
    "        n_correct = 0\n",
    "        total_loss = 0\n",
    "        for i, (inputs, labels, mask) in enumerate(tqdm(loader)):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            mask = mask.to(device)\n",
    "            print(inputs.size())\n",
    "            print(len(mask), len(mask[0]))\n",
    "            print(labels.size())\n",
    "            loss, logits = model(inputs, labels=labels, attention_mask=mask)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.data\n",
    "            n_correct += flat_accuracy(logits, labels)\n",
    "\n",
    "        print('epoch: ', epoch)\n",
    "        print('[train]\\tloss: %f accuracy: %f' % (loss, n_correct/ds_size))\n",
    "\n",
    "        test_loss, test_acc = evaluate(model, test_loader)\n",
    "        print('[test]\\tloss: %f accuracy: %f' % (test_loss, test_acc))\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "class Mydatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = pad_sequence(data, batch_first=True)\n",
    "        max_len = len(self.data[0])\n",
    "        self.mask = torch.tensor([[1]*len(x)+[0]*(max_len-len(x)) for x in data])\n",
    "        self.labels = torch.tensor(labels).long()\n",
    "        self.datanum = len(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label = self.labels[idx]\n",
    "        mask = self.mask[idx]\n",
    "        return out_data, out_label, mask\n",
    "\n",
    "\n",
    "def normalize(doc):\n",
    "    doc = re.sub(r\"[',.]\", '', doc)   # 記号を削除\n",
    "    doc = re.sub(r\" {2,}\", ' ', doc)  # 2回以上続くスペースを削除\n",
    "    doc = re.sub(r\" *?$\", '', doc)    # 行頭と行末のスペースを削除\n",
    "    doc = re.sub(r\"^ *?\", '', doc)\n",
    "    doc = doc.lower()                 # 小文字に統一\n",
    "    return doc\n",
    "\n",
    "\n",
    "def preprocessor(doc):\n",
    "    doc = normalize(doc)\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    tokens = torch.tensor(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def accuracy(pred, label):\n",
    "    pred = np.argmax(pred.data.numpy(), axis=1)  # 行ごとに最大値のインデックスを取得する．\n",
    "    label = label.data.numpy()\n",
    "    return (pred == label).mean()\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    for inputs, labels, lengs in loader:\n",
    "        outputs = model(inputs, lengs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        acc = accuracy(outputs, labels)\n",
    "    return loss.data, acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dw = 300\n",
    "dh = 50\n",
    "L = 4\n",
    "batch_size = 1024\n",
    "columns = ('category', 'title')\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "test = pd.read_csv('../../data/NewsAggregatorDataset/test.txt',\n",
    "                   names=columns, sep='\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10672, 1017])\n",
      "1017\n",
      "torch.Size([1334, 25])\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train['tokens'] = train.title.apply(preprocessor)\n",
    "# test['tokens'] = test.title.apply(preprocessor)\n",
    "\n",
    "# X_train = train['tokens']\n",
    "# X_test = test['tokens']\n",
    "\n",
    "\n",
    "# label2int = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "# Y_train = train.category.map(label2int)\n",
    "# Y_test = test.category.map(label2int)\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "trainset = Mydatasets(X_train, Y_train)\n",
    "testset = Mydatasets(X_test, Y_test)\n",
    "# loader = DataLoader(trainset, batch_size=batch_size)\n",
    "# test_loader = DataLoader(testset, batch_size=testset.__len__())\n",
    "\n",
    "# model = model.to(device)\n",
    "# ds_size = trainset.__len__()\n",
    "\n",
    "# print('train')\n",
    "# trainer(model, optimizer, loader, test_loader, ds_size, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([1, 2, 0])\n",
    "a = tokenizer.convert_tokens_to_ids(['a', '#er'])\n",
    "a = torch.tensor(a)\n",
    "\n",
    "a.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
