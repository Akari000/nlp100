{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 80. ID番号への変換\n",
    "問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に1，2番目に頻出する単語に2，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて0とせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import tokenize, normalize\n",
    "import torch\n",
    "import json\n",
    "\n",
    "\n",
    "def token2id(token, token2id_dic):\n",
    "    if token in token2id_dic:\n",
    "        return token2id_dic[token]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def tokens2ids(tokens, token2id_dic):\n",
    "    tokens = [token2id(token, token2id_dic) for token in tokens]\n",
    "    return torch.tensor(tokens, dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ('category', 'title')\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "\n",
    "\n",
    "docs = [normalize(doc) for doc in train.title.values.tolist()]\n",
    "tokens = [tokenize(doc) for doc in docs]\n",
    "tokens = sum(tokens, [])  # flat list\n",
    "counter = Counter(tokens)\n",
    "\n",
    "token2id_dic = {}\n",
    "vocab_size = len(counter)\n",
    "for index, (token, freq) in enumerate(counter.most_common(), 1):\n",
    "    if freq < 2:\n",
    "        token2id_dic[token] = 0\n",
    "    else:\n",
    "        token2id_dic[token] = index\n",
    "\n",
    "with open('../token2id_dic.json', 'w') as f:\n",
    "    json.dump(token2id_dic, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I am a cat'\n",
    "ids = tokens2ids(tokenize(text), token2id_dic=token2id_dic)\n",
    "print(ids)\n",
    "\n",
    "'''\n",
    "tensor([   0, 3353,   12, 3426])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 81. RNNによる予測\n",
    "ID番号で表現された単語列x=(x1,x2,…,xT)がある．ただし，Tは単語列の長さ，xt∈ℝVは単語のID番号のone-hot表記である（Vは単語の総数である）．再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い，単語列xからカテゴリyを予測するモデルとして，次式を実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from utils import preprocessor, tokens2ids\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn as nn\n",
    "import json\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, output_size, vocab_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb = torch.nn.Embedding(vocab_size, data_size)\n",
    "        self.rnn = torch.nn.RNN(dw, dh, nonlinearity='relu')\n",
    "        self.liner = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, lengs, hidden=None):   # x: (max_len)\n",
    "        x = self.emb(x)                         # x: (max_length, dw)\n",
    "        packed = pack_padded_sequence(\n",
    "            x, lengs, batch_first=True, enforce_sorted=False)\n",
    "        y, hidden = self.rnn(packed, hidden)    # y: (max_len, dh), hidden: (max_len, dh)\n",
    "        y, _ = pad_packed_sequence(y, batch_first=True)\n",
    "        y = y[:, -1, :]\n",
    "        y = self.liner(y)\n",
    "        y = torch.softmax(y, dim=1)\n",
    "        return y, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('token2id_dic.json', 'r') as f:\n",
    "    token2id_dic = json.loads(f.read())\n",
    "dw = 300\n",
    "dh = 50\n",
    "L = 4\n",
    "columns = ('category', 'title')\n",
    "vocab_size = len(token2id_dic)\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "\n",
    "\n",
    "train['tokens'] = train.title.apply(preprocessor)\n",
    "X_train = train.tokens.apply(tokens2ids, token2id_dic=token2id_dic)\n",
    "# X_train[0] = tensor([   8,    0, 2416, 1604, 2143,    5, 1605,    4,  745])\n",
    "\n",
    "lengs = torch.tensor([len(x) for x in X_train])\n",
    "inputs = pad_sequence(X_train, batch_first=True)\n",
    "model = RNN(dw, dh, L, vocab_size)\n",
    "\n",
    "outputs, hidden = model(inputs, lengs)\n",
    "\n",
    "print(outputs.size())\n",
    "print(hidden.size())\n",
    "\n",
    "'''\n",
    "torch.Size([10672, 4])\n",
    "torch.Size([1, 10672, 50])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 82. 確率的勾配降下法による学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題81で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import preprocessor, tokens2ids\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def accuracy(pred, label):\n",
    "    pred = np.argmax(pred.data.numpy(), axis=1)  # 行ごとに最大値のインデックスを取得する．\n",
    "    label = label.data.numpy()\n",
    "    return (pred == label).mean()\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    for inputs, labels, lengs in loader:\n",
    "        outputs = model(inputs, lengs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        acc = accuracy(outputs, labels)\n",
    "    return loss.data, acc\n",
    "\n",
    "\n",
    "def trainer(model, criterion, optimizer, loader, test_loader, ds_size, max_iter=10):\n",
    "    for epoch in range(10):\n",
    "        n_correct = 0\n",
    "        total_loss = 0\n",
    "        for i, (inputs, labels, lengs) in enumerate(tqdm(loader)):\n",
    "            inputs = inputs[:, :max(lengs)]\n",
    "\n",
    "            # with detect_anomaly():\n",
    "            outputs = model(inputs, lengs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.data\n",
    "            outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "            labels = labels.data.numpy()\n",
    "            for output, label in zip(outputs, labels):\n",
    "                if output == label:\n",
    "                    n_correct += 1\n",
    "\n",
    "        print('epoch: ', epoch)\n",
    "        print('[train]\\tloss: %f accuracy: %f' % (loss, n_correct/ds_size))\n",
    "\n",
    "        test_loss, test_acc = evaluate(model, test_loader)\n",
    "        print('[test]\\tloss: %f accuracy: %f' % (test_loss, test_acc))\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, output_size, vocab_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb = nn.Embedding(vocab_size, data_size, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(dw, dh, batch_first=True)\n",
    "        self.liner = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, lengs, hidden=None, cell=None):   # x: (max_len)\n",
    "        x = self.emb(x)                         # x: (max_length, dw)\n",
    "        packed = pack_padded_sequence(\n",
    "            x, lengs, batch_first=True, enforce_sorted=False)\n",
    "        y, (hidden, cell) = self.rnn(packed)    # y: (max_len, dh), hidden: (max_len, dh)\n",
    "        y = self.liner(hidden.view(hidden.shape[1], -1))\n",
    "        y = self.softmax(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Mydatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.lengs = torch.tensor([len(x) for x in data])\n",
    "        self.data = pad_sequence(data, batch_first=True)\n",
    "        self.labels = torch.tensor(labels).long()\n",
    "\n",
    "        self.datanum = len(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label = self.labels[idx]\n",
    "        lengs = self.lengs[idx]\n",
    "        return out_data, out_label, lengs\n",
    "\n",
    "\n",
    "with open('../token2id_dic.json', 'r') as f:\n",
    "    token2id_dic = json.loads(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dw = 300\n",
    "dh = 50\n",
    "L = 4\n",
    "batch_size = 1\n",
    "columns = ('category', 'title')\n",
    "vocab_size = len(token2id_dic)\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "test = pd.read_csv('../../data/NewsAggregatorDataset/test.txt',\n",
    "                   names=columns, sep='\\t')\n",
    "\n",
    "\n",
    "train['tokens'] = train.title.apply(preprocessor)\n",
    "test['tokens'] = test.title.apply(preprocessor)\n",
    "\n",
    "X_train = train.tokens.apply(tokens2ids, token2id_dic=token2id_dic)\n",
    "X_test = test.tokens.apply(tokens2ids, token2id_dic=token2id_dic)\n",
    "\n",
    "label2int = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "Y_train = train.category.map(label2int)\n",
    "Y_test = test.category.map(label2int)\n",
    "\n",
    "\n",
    "model = RNN(dw, dh, L, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "trainset = Mydatasets(X_train, Y_train)\n",
    "testset = Mydatasets(X_test, Y_test)\n",
    "loader = DataLoader(trainset, batch_size=batch_size)\n",
    "test_loader = DataLoader(testset, batch_size=testset.__len__())\n",
    "\n",
    "ds_size = trainset.__len__()\n",
    "\n",
    "trainer(model, criterion, optimizer, loader, test_loader, ds_size, 10)\n",
    "\n",
    "'''\n",
    "epoch:  0\n",
    "[train]\tloss: 0.845764 accuracy: 0.544228\n",
    "[test]\tloss: 1.102350 accuracy: 0.668666\n",
    "epoch:  1\n",
    "[train]\tloss: 0.746207 accuracy: 0.715705\n",
    "[test]\tloss: 0.988419 accuracy: 0.763118\n",
    "epoch:  2\n",
    "[train]\tloss: 0.744244 accuracy: 0.768085\n",
    "[test]\tloss: 0.983985 accuracy: 0.762369\n",
    "epoch:  3\n",
    "[train]\tloss: 0.744022 accuracy: 0.787294\n",
    "[test]\tloss: 0.976032 accuracy: 0.765367\n",
    "epoch:  4\n",
    "[train]\tloss: 0.743966 accuracy: 0.799100\n",
    "[test]\tloss: 0.966806 accuracy: 0.773613\n",
    "epoch:  5\n",
    "[train]\tloss: 0.744006 accuracy: 0.802286\n",
    "[test]\tloss: 0.966757 accuracy: 0.772864\n",
    "epoch:  6\n",
    "[train]\tloss: 0.743976 accuracy: 0.805660\n",
    "[test]\tloss: 0.965994 accuracy: 0.770615\n",
    "epoch:  7\n",
    "[train]\tloss: 0.744010 accuracy: 0.807346\n",
    "[test]\tloss: 0.962968 accuracy: 0.778861\n",
    "epoch:  8\n",
    "[train]\tloss: 0.744325 accuracy: 0.806972\n",
    "[test]\tloss: 0.963270 accuracy: 0.779610\n",
    "epoch:  9\n",
    "[train]\tloss: 0.743826 accuracy: 0.809970\n",
    "[test]\tloss: 0.962994 accuracy: 0.778111\n",
    "Finished Training\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 83. ミニバッチ化・GPU上での学習\n",
    "問題82のコードを改変し，B事例ごとに損失・勾配を計算して学習を行えるようにせよ（Bの値は適当に選べ）．また，GPU上で学習を実行せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, criterion, optimizer, loader, test_loader, ds_size, device, max_iter=10):\n",
    "    for epoch in range(10):\n",
    "        n_correct = 0\n",
    "        total_loss = 0\n",
    "        for i, (inputs, labels, lengs) in enumerate(tqdm(loader)):\n",
    "            inputs = inputs[:, :max(lengs)]\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            lengs = lengs.to(device)\n",
    "\n",
    "            outputs = model(inputs, lengs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.data\n",
    "            outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "            labels = labels.data.numpy()\n",
    "            for output, label in zip(outputs, labels):\n",
    "                if output == label:\n",
    "                    n_correct += 1\n",
    "\n",
    "        print('epoch: ', epoch)\n",
    "        print('[train]\\tloss: %f accuracy: %f' % (loss, n_correct/ds_size))\n",
    "\n",
    "        test_loss, test_acc = evaluate(model, test_loader)\n",
    "        print('[test]\\tloss: %f accuracy: %f' % (test_loss, test_acc))\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "train['tokens'] = train.title.apply(preprocessor)\n",
    "test['tokens'] = test.title.apply(preprocessor)\n",
    "\n",
    "X_train = train.tokens.apply(tokens2ids, token2id_dic=token2id_dic)\n",
    "X_test = test.tokens.apply(tokens2ids, token2id_dic=token2id_dic)\n",
    "\n",
    "label2int = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "Y_train = train.category.map(label2int)\n",
    "Y_test = test.category.map(label2int)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNN(dw, dh, L, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "trainset = Mydatasets(X_train, Y_train)\n",
    "testset = Mydatasets(X_test, Y_test)\n",
    "loader = DataLoader(trainset, batch_size=batch_size)\n",
    "test_loader = DataLoader(testset, batch_size=testset.__len__())\n",
    "\n",
    "model = model.to(device)\n",
    "ds_size = trainset.__len__()\n",
    "\n",
    "trainer(model, criterion, optimizer, loader, test_loader, ds_size, device, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 84. 単語ベクトルの導入\n",
    "事前学習済みの単語ベクトル（例えば，Google Newsデータセット（約1,000億単語）での学習済み単語ベクトル）で単語埋め込みemb(x)を初期化し，学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/Users/hagaakari/.pyenv/versions/3.8.0/lib/python3.8/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n  from pandas import Panel\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import preprocessor, tokens2ids\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "googlenews = KeyedVectors.load_word2vec_format(\n",
    "    '../../data/GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokens2vec(tokens):\n",
    "    vec = []\n",
    "    for token in tokens:\n",
    "        if token in googlenews:\n",
    "            vec.append(googlenews[token])\n",
    "        else:\n",
    "            vec.append(np.array([0]*dw))\n",
    "    return torch.tensor(vec)\n",
    "\n",
    "\n",
    "def accuracy(pred, label):\n",
    "    pred = np.argmax(pred.data.numpy(), axis=1)  # 行ごとに最大値のインデックスを取得する．\n",
    "    label = label.data.numpy()\n",
    "    return (pred == label).mean()\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    for inputs, labels, lengs in loader:\n",
    "        outputs = model(inputs, lengs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        acc = accuracy(outputs, labels)\n",
    "    return loss.data, acc\n",
    "\n",
    "\n",
    "def trainer(model, criterion, optimizer, loader, test_loader, ds_size, device, max_iter=10):\n",
    "    for epoch in range(10):\n",
    "        n_correct = 0\n",
    "        total_loss = 0\n",
    "        for i, (inputs, labels, lengs) in enumerate(tqdm(loader)):\n",
    "            inputs = inputs[:, :max(lengs)]\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            lengs = lengs.to(device)\n",
    "\n",
    "            outputs = model(inputs, lengs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.data\n",
    "            outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "            labels = labels.data.numpy()\n",
    "            for output, label in zip(outputs, labels):\n",
    "                if output == label:\n",
    "                    n_correct += 1\n",
    "\n",
    "        print('epoch: ', epoch)\n",
    "        print('[train]\\tloss: %f accuracy: %f' % (loss, n_correct/ds_size))\n",
    "\n",
    "        test_loss, test_acc = evaluate(model, test_loader)\n",
    "        print('[test]\\tloss: %f accuracy: %f' % (test_loss, test_acc))\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, output_size, vocab_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb = nn.Embedding(vocab_size, data_size, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(dw, dh, batch_first=True)\n",
    "        self.liner = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, lengs, hidden=None, cell=None):   # x: (max_len)\n",
    "        # x = self.emb(x)                         # x: (max_length, dw)\n",
    "        packed = pack_padded_sequence(\n",
    "            x, lengs, batch_first=True, enforce_sorted=False)\n",
    "        y, (hidden, cell) = self.rnn(packed)    # y: (max_len, dh), hidden: (max_len, dh)\n",
    "        y = self.liner(hidden.view(hidden.shape[1], -1))\n",
    "        y = self.softmax(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Mydatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.lengs = torch.tensor([len(x) for x in data])\n",
    "        self.data = pad_sequence(data, batch_first=True)\n",
    "        self.labels = torch.tensor(labels).long()\n",
    "\n",
    "        self.datanum = len(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label = self.labels[idx]\n",
    "        lengs = self.lengs[idx]\n",
    "        return out_data, out_label, lengs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 10672/10672 [00:05<00:00, 2055.51it/s]\n100%|██████████| 1334/1334 [00:00<00:00, 2106.54it/s]\n100%|██████████| 1334/1334 [00:07<00:00, 167.28it/s]\n  0%|          | 0/1334 [00:00<?, ?it/s]epoch:  0\n[train]\tloss: 1.306197 accuracy: 0.418103\n[test]\tloss: 1.296853 accuracy: 0.437781\n100%|██████████| 1334/1334 [00:07<00:00, 174.01it/s]\n  0%|          | 0/1334 [00:00<?, ?it/s]epoch:  1\n[train]\tloss: 1.278086 accuracy: 0.422695\n[test]\tloss: 1.258812 accuracy: 0.456522\n100%|██████████| 1334/1334 [00:07<00:00, 175.01it/s]\n  0%|          | 0/1334 [00:00<?, ?it/s]epoch:  2\n[train]\tloss: 1.253694 accuracy: 0.561282\n[test]\tloss: 1.212802 accuracy: 0.682909\n100%|██████████| 1334/1334 [00:08<00:00, 157.28it/s]\n  0%|          | 0/1334 [00:00<?, ?it/s]epoch:  3\n[train]\tloss: 1.220054 accuracy: 0.735757\n[test]\tloss: 1.014457 accuracy: 0.766867\n100%|██████████| 1334/1334 [00:09<00:00, 133.52it/s]\n  0%|          | 0/1334 [00:00<?, ?it/s]epoch:  4\n[train]\tloss: 1.233165 accuracy: 0.765555\n[test]\tloss: 0.978168 accuracy: 0.774363\n100%|██████████| 1334/1334 [00:11<00:00, 115.11it/s]\n  0%|          | 0/1334 [00:00<?, ?it/s]epoch:  5\n[train]\tloss: 1.236411 accuracy: 0.772114\n[test]\tloss: 0.968411 accuracy: 0.780360\n100%|██████████| 1334/1334 [00:13<00:00, 99.25it/s]\nepoch:  6\n[train]\tloss: 1.238182 accuracy: 0.776143\n  1%|          | 9/1334 [00:00<00:15, 83.65it/s][test]\tloss: 0.964659 accuracy: 0.781859\n100%|██████████| 1334/1334 [00:10<00:00, 124.35it/s]\n  0%|          | 0/1334 [00:00<?, ?it/s]epoch:  7\n[train]\tloss: 1.238690 accuracy: 0.778579\n[test]\tloss: 0.959650 accuracy: 0.784108\n100%|██████████| 1334/1334 [00:10<00:00, 122.86it/s]\n  0%|          | 0/1334 [00:00<?, ?it/s]epoch:  8\n[train]\tloss: 1.240388 accuracy: 0.781765\n[test]\tloss: 0.958462 accuracy: 0.784108\n100%|██████████| 1334/1334 [00:09<00:00, 141.97it/s]\nepoch:  9\n[train]\tloss: 1.241063 accuracy: 0.783265\n[test]\tloss: 0.957121 accuracy: 0.785607\nFinished Training\n"
    }
   ],
   "source": [
    "with open('../token2id_dic.json', 'r') as f:\n",
    "    token2id_dic = json.loads(f.read())\n",
    "\n",
    "dw = 300\n",
    "dh = 50\n",
    "L = 4\n",
    "batch_size = 8\n",
    "columns = ('category', 'title')\n",
    "vocab_size = len(token2id_dic)\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "test = pd.read_csv('../../data/NewsAggregatorDataset/test.txt',\n",
    "                   names=columns, sep='\\t')\n",
    "\n",
    "\n",
    "train['tokens'] = train.title.apply(preprocessor)\n",
    "test['tokens'] = test.title.apply(preprocessor)\n",
    "\n",
    "X_train = train.tokens.progress_apply(tokens2vec).values.tolist()\n",
    "X_test = test.tokens.progress_apply(tokens2vec).values.tolist()\n",
    "# X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "\n",
    "pad_sequence(X_train, batch_first=True)\n",
    "label2int = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "Y_train = train.category.map(label2int)\n",
    "Y_test = test.category.map(label2int)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNN(dw, dh, L, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "trainset = Mydatasets(X_train, Y_train)\n",
    "testset = Mydatasets(X_test, Y_test)\n",
    "loader = DataLoader(trainset, batch_size=batch_size)\n",
    "test_loader = DataLoader(testset, batch_size=testset.__len__())\n",
    "\n",
    "model = model.to(device)\n",
    "ds_size = trainset.__len__()\n",
    "\n",
    "trainer(model, criterion, optimizer, loader, test_loader, ds_size, device, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 85. 双方向RNN・多層化\n",
    "順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし，モデルを学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalRNN(nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, output_size, vocab_size):\n",
    "        super(BidirectionalRNN, self).__init__()\n",
    "        self.emb = torch.nn.Embedding(vocab_size, data_size)\n",
    "        self.rnn1 = torch.nn.RNN(data_size, hidden_size, nonlinearity='relu', bidirectional=True)\n",
    "        self.rnn2 = torch.nn.RNN(2*hidden_size, hidden_size, nonlinearity='relu', bidirectional=True)\n",
    "        self.liner = nn.Linear(2*hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, data, last_hidden):           # data: (max_len)\n",
    "        data = self.emb(data)                   # data: (max_length, dw)\n",
    "        y, hidden = self.rnn1(data, last_hidden)     # y: (max_len, dh), hidden: (max_len, dh)\n",
    "        y, hidden = self.rnn2(y, hidden)\n",
    "        y = y[:,-1,:]\n",
    "        y = self.liner(y)\n",
    "        y = torch.softmax(y, dim=1)\n",
    "        return y, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BidirectionalRNN(dw, dh, L, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "X_train = pad_sequence(X_train, batch_first=True)\n",
    "max_len = len(X_train[0])\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=1024, shuffle=True)\n",
    "\n",
    "model   = model.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    hidden = torch.zeros(2, max_len, dh, dtype=torch.float32)\n",
    "    n_correct = 0\n",
    "    total_loss = 0\n",
    "    for inputs, labels in tqdm(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() # パラメータを更新\n",
    "\n",
    "        total_loss += loss.data \n",
    "        outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "        labels = labels.data.numpy()\n",
    "        hidden = hidden.detach()\n",
    "        for output, label in zip(outputs, labels):\n",
    "            if output == label:\n",
    "                n_correct += 1\n",
    "        \n",
    "    print('epoch: %d loss: %f accuracy: %f' % (epoch, loss, n_correct/dataset_size))\n",
    "\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 86. 畳み込みニューラルネットワーク (CNN)\n",
    "ID番号で表現された単語列x=(x1,x2,…,xT)がある．ただし，Tは単語列の長さ，xt∈ℝVは単語のID番号のone-hot表記である（Vは単語の総数である）．畳み込みニューラルネットワーク（CNN: Convolutional Neural Network）を用い，単語列xからカテゴリyを予測するモデルを実装せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "dw = 300\n",
    "dh = 50\n",
    "L = 4\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, output_size, vocab_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.emb = torch.nn.Embedding(vocab_size, data_size)\n",
    "        self.conv = torch.nn.Conv1d(data_size, hidden_size, 3, padding=1) # in_channels, out_channels, kernel_sizes\n",
    "        self.pool = torch.nn.MaxPool1d(120)\n",
    "        self.liner_px = nn.Linear(data_size*3, hidden_size)\n",
    "        self.liner_yc = nn.Linear(hidden_size, output_size)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):                       # x: (max_len)\n",
    "        x = self.emb(x)                         # x: (max_length, dw)\n",
    "        x = x.view(-1, x.shape[2], x.shape[1])  # x: (dw, max_length)\n",
    "        x = self.conv(x)                        # 畳み込み x: (dh, max_len)\n",
    "        p = self.act(x)\n",
    "        c = self.pool(p)                        # c: (dh, 1)\n",
    "        c = c.view(c.shape[0], c.shape[1])      # c: (1, dh)\n",
    "        y = self.liner_yc(c)                    # c: (1, L)\n",
    "        y = torch.softmax(y, dim=1)\n",
    "        return y\n",
    "\n",
    "\n",
    "X_train = train.tokens.apply(tokens2ids)\n",
    "max_len = train.tokens.apply(len).max()\n",
    "model = CNN(dw, dh, L, vocab_size)\n",
    "\n",
    "inputs = pad_sequence(X_train, batch_first=True)\n",
    "\n",
    "outputs = model(inputs[:1])\n",
    "print('output.size', outputs.size())\n",
    "print(outputs)\n",
    "\n",
    "'''\n",
    "output.size torch.Size([1, 4])\n",
    "tensor([[0.1083, 0.2877, 0.4019, 0.2021]], grad_fn=<SoftmaxBackward>)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 87. 確率的勾配降下法によるCNNの学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題86で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ('category', 'title')\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "test = pd.read_csv('../../data/NewsAggregatorDataset/test.txt',\n",
    "                   names=columns, sep='\\t')\n",
    "\n",
    "train['tokens'] = train.title.apply(preprocessor)\n",
    "test['tokens'] = test.title.apply(preprocessor)\n",
    "\n",
    "X_train = train.tokens.apply(tokens2ids)\n",
    "X_train = pad_sequence(X_train, batch_first=True)\n",
    "X_test = test.tokens.apply(tokens2ids)\n",
    "X_test = pad_sequence(X_test, batch_first=True)\n",
    "\n",
    "label2int = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "Y_train = train.category.map(label2int)\n",
    "Y_test = test.category.map(label2int)\n",
    "Y_train = torch.tensor(Y_train).long()\n",
    "Y_test = torch.tensor(Y_test).long()\n",
    "\n",
    "max_len = train.tokens.apply(len).max()\n",
    "dataset_size = len(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ('category', 'title')\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "test = pd.read_csv('../../data/NewsAggregatorDataset/test.txt',\n",
    "                   names=columns, sep='\\t')\n",
    "\n",
    "train['tokens'] = train.title.apply(preprocessor)\n",
    "test['tokens'] = test.title.apply(preprocessor)\n",
    "\n",
    "X_train = train.tokens.apply(tokens2ids)\n",
    "X_train = pad_sequence(X_train, batch_first=True)\n",
    "X_test = test.tokens.apply(tokens2ids)\n",
    "X_test = pad_sequence(X_test, batch_first=True)\n",
    "\n",
    "label2int = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "Y_train = train.category.map(label2int)\n",
    "Y_test = test.category.map(label2int)\n",
    "Y_train = torch.tensor(Y_train).long()\n",
    "Y_test = torch.tensor(Y_test).long()\n",
    "\n",
    "max_len = train.tokens.apply(len).max()\n",
    "dataset_size = len(train)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN(dw, dh, L, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=1024, shuffle=True)\n",
    "\n",
    "model   = model.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    n_correct = 0\n",
    "    total_loss = 0\n",
    "    for inputs, labels in tqdm(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() # パラメータを更新\n",
    "        \n",
    "        total_loss += loss.data \n",
    "        outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "        labels = labels.data.numpy()\n",
    "        for output, label in zip(outputs, labels):\n",
    "            if output == label:\n",
    "                n_correct += 1\n",
    "        \n",
    "    print('epoch: %d loss: %f accuracy: %f' % (epoch, loss, n_correct/dataset_size))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "'''\n",
    "100%|██████████| 11/11 [00:14<00:00,  1.36s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 0 loss: 1.324250 accuracy: 0.366567\n",
    "100%|██████████| 11/11 [00:15<00:00,  1.42s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 1 loss: 1.291704 accuracy: 0.397395\n",
    "100%|██████████| 11/11 [00:18<00:00,  1.69s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 2 loss: 1.272223 accuracy: 0.456241\n",
    "100%|██████████| 11/11 [00:16<00:00,  1.49s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 3 loss: 1.263339 accuracy: 0.484726\n",
    "100%|██████████| 11/11 [00:23<00:00,  2.10s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 4 loss: 1.244924 accuracy: 0.500281\n",
    "100%|██████████| 11/11 [00:16<00:00,  1.52s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 5 loss: 1.251264 accuracy: 0.479854\n",
    "100%|██████████| 11/11 [00:14<00:00,  1.35s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 6 loss: 1.244529 accuracy: 0.523145\n",
    "100%|██████████| 11/11 [00:15<00:00,  1.39s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 7 loss: 1.243323 accuracy: 0.515555\n",
    "100%|██████████| 11/11 [00:21<00:00,  1.98s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 8 loss: 1.242815 accuracy: 0.535139\n",
    "100%|██████████| 11/11 [00:14<00:00,  1.36s/it]epoch: 9 loss: 1.226233 accuracy: 0.531297\n",
    "Finished Training\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 88. パラメータチューニング\n",
    "問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 89. 事前学習済み言語モデルからの転移学習\n",
    "事前学習済み言語モデル（例えばBERTなど）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', \n",
    "                                                      num_labels=4,\n",
    "                                                      output_attentions = False,\n",
    "                                                      output_hidden_states = False,)\n",
    "\n",
    "\n",
    "# inputs = X_train[0].unsqueeze(0)\n",
    "# # labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "# labels = Y_train[0]\n",
    "# outputs = model(inputs, labels=labels)\n",
    "\n",
    "# loss = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == label)\n",
    "\n",
    "\n",
    "def trainer(model, criterion, optimizer, loader, test_loader, ds_size, device, max_iter=10):\n",
    "    for epoch in range(10):\n",
    "        n_correct = 0\n",
    "        total_loss = 0\n",
    "        for i, (inputs, labels, lengs) in enumerate(tqdm(loader)):\n",
    "            inputs = inputs[:, :max(lengs)]\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            lengs = lengs.to(device)\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss, logits = outputs[:2]\n",
    "            \n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.data\n",
    "            n_correct += flat_accuracy(logits, label_ids)\n",
    "            outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "            # labels = labels.data.numpy()\n",
    "            # for output, label in zip(outputs, labels):\n",
    "            #     if output == label:\n",
    "            #         n_correct += 1\n",
    "\n",
    "        print('epoch: ', epoch)\n",
    "        print('[train]\\tloss: %f accuracy: %f' % (loss, n_correct/ds_size))\n",
    "\n",
    "        test_loss, test_acc = evaluate(model, test_loader)\n",
    "        print('[test]\\tloss: %f accuracy: %f' % (test_loss, test_acc))\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "class Mydatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.lengs = torch.tensor([len(x) for x in data])\n",
    "        self.data = pad_sequence(data, batch_first=True)\n",
    "        self.labels = torch.tensor(labels).long()\n",
    "\n",
    "        self.datanum = len(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label = self.labels[idx]\n",
    "        lengs = self.lengs[idx]\n",
    "        return out_data, out_label, lengs\n",
    "\n",
    "\n",
    "def normalize(doc):\n",
    "    doc = re.sub(r\"[',.]\", '', doc)   # 記号を削除\n",
    "    doc = re.sub(r\" {2,}\", ' ', doc)  # 2回以上続くスペースを削除\n",
    "    doc = re.sub(r\" *?$\", '', doc)    # 行頭と行末のスペースを削除\n",
    "    doc = re.sub(r\"^ *?\", '', doc)\n",
    "    doc = doc.lower()                 # 小文字に統一\n",
    "    return doc\n",
    "\n",
    "\n",
    "def preprocessor(doc):\n",
    "    doc = normalize(doc)\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-228c22d585a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokens2ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import tokens2ids\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "dw = 300\n",
    "dh = 50\n",
    "L = 4\n",
    "batch_size = 1024\n",
    "columns = ('category', 'title')\n",
    "vocab_size = len(token2id_dic)\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "test = pd.read_csv('../../data/NewsAggregatorDataset/test.txt',\n",
    "                   names=columns, sep='\\t')\n",
    "\n",
    "\n",
    "train['tokens'] = train.title.apply(preprocessor)\n",
    "test['tokens'] = test.title.apply(preprocessor)\n",
    "\n",
    "X_train = train.tokens.apply(tokens2ids, token2id_dic=token2id_dic)\n",
    "X_test = test.tokens.apply(tokens2ids, token2id_dic=token2id_dic)\n",
    "\n",
    "label2int = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "Y_train = train.category.map(label2int)\n",
    "Y_test = test.category.map(label2int)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = RNN(dw, dh, L, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "trainset = Mydatasets(X_train, Y_train)\n",
    "testset = Mydatasets(X_test, Y_test)\n",
    "loader = DataLoader(trainset, batch_size=batch_size)\n",
    "test_loader = DataLoader(testset, batch_size=testset.__len__())\n",
    "\n",
    "model = model.to(device)\n",
    "ds_size = trainset.__len__()\n",
    "\n",
    "trainer(model, criterion, optimizer, loader, test_loader, ds_size, device, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}