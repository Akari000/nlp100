{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 80. ID番号への変換\n",
    "問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に1，2番目に頻出する単語に2，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて0とせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def tokenize(doc):\n",
    "    tokens = doc.split(' ')\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def normalize(doc):\n",
    "    doc = re.sub(r\"[',.]\", '', doc)   # 記号を削除\n",
    "    doc = re.sub(r\" {2,}\", ' ', doc)  # 2回以上続くスペースを削除\n",
    "    doc = re.sub(r\" *?$\", '', doc)    # 行頭と行末のスペースを削除\n",
    "    doc = re.sub(r\"^ *?\", '', doc)\n",
    "    doc = doc.lower()                 # 小文字に統一\n",
    "    return doc\n",
    "\n",
    "\n",
    "def token2id(token):\n",
    "    return token2id_dic[token]\n",
    "\n",
    "\n",
    "columns = ('category', 'title')\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "\n",
    "\n",
    "docs = [normalize(doc) for doc in train.title.values.tolist()]\n",
    "tokens = [tokenize(doc) for doc in docs]\n",
    "tokens = sum(tokens, [])  # flat list\n",
    "counter = Counter(tokens)\n",
    "\n",
    "token2id_dic = {}\n",
    "vocab_size = len(counter)\n",
    "for index, (token, freq) in enumerate(counter.most_common(), 1):\n",
    "    if freq < 2:\n",
    "        token2id_dic[token] = 0\n",
    "    else:\n",
    "        token2id_dic[token] = index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "16901"
     },
     "metadata": {},
     "execution_count": 175
    }
   ],
   "source": [
    "token2id('the')\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 81. RNNによる予測\n",
    "ID番号で表現された単語列x=(x1,x2,…,xT)がある．ただし，Tは単語列の長さ，xt∈ℝVは単語のID番号のone-hot表記である（Vは単語の総数である）．再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い，単語列xからカテゴリyを予測するモデルとして，次式を実装せよ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(doc):\n",
    "    doc = normalize(doc)    \n",
    "    tokens = tokenize(doc)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokens2ids(tokens):\n",
    "    tokens = [token2id(token) for token in tokens]\n",
    "    return torch.tensor(tokens, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([   8,    0, 2416, 1604, 2143,    5, 1605,    4,  745])"
     },
     "metadata": {},
     "execution_count": 177
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "train['tokens'] = train.title.apply(preprocessor)\n",
    "X_train = train.tokens.apply(tokens2ids)\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "dw = 300\n",
    "dh = 50\n",
    "L = 4\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, output_size, vocab_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.emb = torch.nn.Embedding(vocab_size, data_size)\n",
    "        self.rnn = torch.nn.RNN(dw, dh, nonlinearity='relu')\n",
    "        self.liner = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, data, last_hidden):           # data: (max_len)\n",
    "        data = self.emb(data)                       # data: (max_length, dw)\n",
    "        y, hidden = self.rnn(data, last_hidden)     # y: (max_len, dh), hidden: (max_len, dh)\n",
    "        y = y[:,-1,:]\n",
    "        y = self.liner(y)\n",
    "        y = torch.softmax(y, dim=1)\n",
    "        return y, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([10672, 4])\ntorch.Size([1, 121, 50])\n"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "max_len = train.tokens.apply(len).max()\n",
    "model = RNN(dw, dh, L, vocab_size)\n",
    "\n",
    "label2int = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "Y_train = train.category.map(label2int)\n",
    "Y_train = torch.tensor(Y_train).int()\n",
    "\n",
    "inputs = X_train\n",
    "labels = Y_train\n",
    "\n",
    "\n",
    "inputs = pad_sequence(inputs, batch_first=True)\n",
    "h0 = torch.zeros(1, 121, dh, dtype=torch.float32)\n",
    "\n",
    "outputs, hidden = model(inputs, h0)\n",
    "print(outputs.size())\n",
    "print(hidden.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}