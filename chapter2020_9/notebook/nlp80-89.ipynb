{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 80. ID番号への変換\n",
    "問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に1，2番目に頻出する単語に2，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて0とせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "/Users/hagaakari/.pyenv/versions/3.8.0/lib/python3.8/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n  from pandas import Panel\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def tokenize(doc):\n",
    "    tokens = doc.split(' ')\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def normalize(doc):\n",
    "    doc = re.sub(r\"[',.]\", '', doc)   # 記号を削除\n",
    "    doc = re.sub(r\" {2,}\", ' ', doc)  # 2回以上続くスペースを削除\n",
    "    doc = re.sub(r\" *?$\", '', doc)    # 行頭と行末のスペースを削除\n",
    "    doc = re.sub(r\"^ *?\", '', doc)\n",
    "    doc = doc.lower()                 # 小文字に統一\n",
    "    return doc\n",
    "\n",
    "\n",
    "def token2id(token):\n",
    "    if token in token2id_dic:\n",
    "        return token2id_dic[token]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "columns = ('category', 'title')\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "\n",
    "\n",
    "docs = [normalize(doc) for doc in train.title.values.tolist()]\n",
    "tokens = [tokenize(doc) for doc in docs]\n",
    "tokens = sum(tokens, [])  # flat list\n",
    "counter = Counter(tokens)\n",
    "\n",
    "token2id_dic = {}\n",
    "vocab_size = len(counter)\n",
    "for index, (token, freq) in enumerate(counter.most_common(), 1):\n",
    "    if freq < 2:\n",
    "        token2id_dic[token] = 0\n",
    "    else:\n",
    "        token2id_dic[token] = index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id('the')\n",
    "'''\n",
    "3\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 81. RNNによる予測\n",
    "ID番号で表現された単語列x=(x1,x2,…,xT)がある．ただし，Tは単語列の長さ，xt∈ℝVは単語のID番号のone-hot表記である（Vは単語の総数である）．再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い，単語列xからカテゴリyを予測するモデルとして，次式を実装せよ...\n",
    "[参考](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "def preprocessor(doc):\n",
    "    doc = normalize(doc)    \n",
    "    tokens = tokenize(doc)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokens2ids(tokens):\n",
    "    tokens = [token2id(token) for token in tokens]\n",
    "    return torch.tensor(tokens, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = 300\n",
    "dh = 50\n",
    "L = 4\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, output_size, vocab_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb = torch.nn.Embedding(vocab_size, data_size)\n",
    "        self.rnn = torch.nn.RNN(dw, dh, nonlinearity='relu')\n",
    "        self.liner = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, lengs, hidden=None): # x: (max_len)\n",
    "        x = self.emb(x)                       # x: (max_length, dw)\n",
    "        packed = pack_padded_sequence(\n",
    "            x, lengs, batch_first=True, enforce_sorted=False)\n",
    "        y, hidden = self.rnn(packed, hidden)  # y: (max_len, dh), hidden: (max_len, dh)\n",
    "        y, _ = pad_packed_sequence(y, batch_first=True)\n",
    "        y = y[:,-1,:]\n",
    "        print('rnn output', y)\n",
    "        y = self.liner(y)\n",
    "        print('liner output', y)\n",
    "        y = torch.softmax(y, dim=1)\n",
    "        return y, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([10672, 4])\ntorch.Size([1, 10672, 50])\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\ntorch.Size([10672, 4])\\ntorch.Size([1, 10672, 50])\\n'"
     },
     "metadata": {},
     "execution_count": 260
    }
   ],
   "source": [
    "train['tokens'] = train.title.apply(preprocessor)\n",
    "X_train = train.tokens.apply(tokens2ids)\n",
    "\n",
    "# tensor([   8,    0, 2416, 1604, 2143,    5, 1605,    4,  745])\n",
    "\n",
    "lengs = torch.tensor([len(x) for x in X_train])\n",
    "inputs = pad_sequence(X_train, batch_first=True)\n",
    "model = RNN(dw, dh, L, vocab_size)\n",
    "\n",
    "outputs, hidden = model(inputs, lengs)\n",
    "\n",
    "print(outputs.size())\n",
    "print(hidden.size())\n",
    "\n",
    "'''\n",
    "torch.Size([10672, 4])\n",
    "torch.Size([1, 10672, 50])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "input must have 2 dimensions, got 1",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-322-95b25dcc6ab4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m             torch.tensor([[1.,2,3,0],[3,4,5,6]]), [3,4], batch_first=True, enforce_sorted=False)\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# hidden = torch.zeros(1, 4, 4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# y, lengs = pad_packed_sequence(packed, batch_first=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# lengs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0m_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_rnn_impls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# type: (Tensor, Tensor, Optional[Tensor]) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mexpected_input_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_input_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    165\u001b[0m                 'input must have {} dimensions, got {}'.format(\n\u001b[1;32m    166\u001b[0m                     expected_input_dim, input.dim()))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input must have 2 dimensions, got 1"
     ]
    }
   ],
   "source": [
    "rnn = torch.nn.RNN(dw, dh, nonlinearity='relu')\n",
    "packed = pack_padded_sequence(\n",
    "            torch.tensor([[1.,2,3,0],[3,4,5,6]]), [3,4], batch_first=True, enforce_sorted=False)\n",
    "# hidden = torch.zeros(1, 4, 4)\n",
    "y, hidden = rnn(packed) \n",
    "# y, lengs = pad_packed_sequence(packed, batch_first=True)\n",
    "# lengs\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 82. 確率的勾配降下法による学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題81で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.autograd import detect_anomaly\n",
    "import numpy as np\n",
    "\n",
    "columns = ('category', 'title')\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "test = pd.read_csv('../../data/NewsAggregatorDataset/test.txt',\n",
    "                   names=columns, sep='\\t')\n",
    "\n",
    "train['tokens'] = train.title.apply(preprocessor)\n",
    "test['tokens'] = test.title.apply(preprocessor)\n",
    "\n",
    "X_train = train.tokens.apply(tokens2ids)\n",
    "X_test = test.tokens.apply(tokens2ids)\n",
    "\n",
    "label2int = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "Y_train = train.category.map(label2int)\n",
    "Y_test = test.category.map(label2int)\n",
    "\n",
    "\n",
    "class Mydatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.lengs = torch.tensor([len(x) for x in data])\n",
    "        self.data = pad_sequence(data, batch_first=True)\n",
    "        self.labels = torch.tensor(labels).long()\n",
    "\n",
    "        self.datanum = len(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label = self.labels[idx]\n",
    "        lengs = self.lengs[idx]\n",
    "        return out_data, out_label, lengs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "64%|██████▍   | 6835/10672 [07:38<04:17, 14.90it/s]\n"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Function 'LogSoftmaxBackward' returned nan values in its 0th output.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-274-9cf5867cbb4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# パラメータを更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'LogSoftmaxBackward' returned nan values in its 0th output."
     ]
    }
   ],
   "source": [
    "# 評価データでも正解率を求める．\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNN(dw, dh, L, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "# X_train = pad_sequence(X_train, batch_first=True)\n",
    "\n",
    "# ds = TensorDataset(X_train, Y_train)\n",
    "trainset = Mydatasets(X_train, Y_train)\n",
    "loader = DataLoader(trainset, batch_size=1)\n",
    "\n",
    "model   = model.to(device)\n",
    "ds_size = len(loader)\n",
    "nan_inputs = 0\n",
    "hidden = None\n",
    "for epoch in range(10):\n",
    "    n_correct = 0\n",
    "    total_loss = 0\n",
    "    for inputs, label, lengs in tqdm(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        label = label.to(device)\n",
    "        lengs = lengs.to(device)\n",
    "        with detect_anomaly():\n",
    "          outputs, hidden = model(inputs, lengs, hidden)\n",
    "          loss = criterion(outputs, label)\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step() # パラメータを更新\n",
    "\n",
    "          total_loss += loss.data \n",
    "          outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "          label = label.data.numpy()\n",
    "          hidden = hidden.detach()\n",
    "          if torch.isnan(loss):\n",
    "            print('loss\\t', loss, 'x\\t', inputs, lengs, 'hidden', hidden)\n",
    "            nan_inputs = inputs\n",
    "            break\n",
    "          \n",
    "          if outputs == label:\n",
    "              n_correct += 1\n",
    "    \n",
    "    print('epoch: %d loss: %f accuracy: %f' % (\n",
    "      epoch, loss, n_correct/ds_size))\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(-2147483648, dtype=torch.int32)"
     },
     "metadata": {},
     "execution_count": 283
    }
   ],
   "source": [
    "# len(loader)\n",
    "# sum([torch.isnan(x) for x in X_trai\n",
    "hidden[0][0][0].data.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "pandas.core.series.Series"
     },
     "metadata": {},
     "execution_count": 146
    }
   ],
   "source": [
    "'''loss\t tensor(nan, grad_fn=<NllLossBackward>) x\t tensor([   3,  231, 5154,    1,    3,    0,  150,    4,  607, 1850,  132,  183,\n",
    "         223,    0, 2346, 3080,   21, 3351,    0,    0,  254, 3637,    2,    0,\n",
    "         776, 1254, 3928,    1,  135,   35, 4355,  183,  223,    2,    0,    3,\n",
    "           0,  724,   38,  118, 2302,   14,    3, 7945,   43, 8340,    0,    0,\n",
    "           3,  607,  976,    0,   31, 2744,   49, 1864, 3549, 4541,    0, 1254,\n",
    "          10, 4343, 5533, 4701, 8197,   14,  183,  223,   24,    3,  607,  314,\n",
    "           0,  939, 4459, 5782,  128,    0,    0,    0,  168, 1302,    0, 1118,\n",
    "        5019, 3467,    1,  252,    0, 1302,   78,    1,   26,  200,   98,   80,\n",
    "        1376,  129, 5850,   84, 2483,    0,    0,    0,  148, 1001,    1, 7608,\n",
    "          38,    4,   12,  148,    0,    0,   10,    3,    0, 1588,    5, 4887,\n",
    "           0])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "121"
     },
     "metadata": {},
     "execution_count": 159
    }
   ],
   "source": [
    "a = [   3,  231, 5154,    1,    3,    0,  150,    4,  607, 1850,  132,  183,\n",
    "         223,    0, 2346, 3080,   21, 3351,    0,    0,  254, 3637,    2,    0,\n",
    "         776, 1254, 3928,    1,  135,   35, 4355,  183,  223,    2,    0,    3,\n",
    "           0,  724,   38,  118, 2302,   14,    3, 7945,   43, 8340,    0,    0,\n",
    "           3,  607,  976,    0,   31, 2744,   49, 1864, 3549, 4541,    0, 1254,\n",
    "          10, 4343, 5533, 4701, 8197,   14,  183,  223,   24,    3,  607,  314,\n",
    "           0,  939, 4459, 5782,  128,    0,    0,    0,  168, 1302,    0, 1118,\n",
    "        5019, 3467,    1,  252,    0, 1302,   78,    1,   26,  200,   98,   80,\n",
    "        1376,  129, 5850,   84, 2483,    0,    0,    0,  148, 1001,    1, 7608,\n",
    "          38,    4,   12,  148,    0,    0,   10,    3,    0, 1588,    5, 4887,\n",
    "           0]\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 83. ミニバッチ化・GPU上での学習\n",
    "問題82のコードを改変し，B事例ごとに損失・勾配を計算して学習を行えるようにせよ（Bの値は適当に選べ）．また，GPU上で学習を実行せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNN(dw, dh, L, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "X_train = pad_sequence(X_train, batch_first=True)\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=1024, shuffle=True)\n",
    "\n",
    "model   = model.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    hidden = torch.zeros(1, max_len, dh, dtype=torch.float32)\n",
    "    n_correct = 0\n",
    "    total_loss = 0\n",
    "    for inputs, labels in tqdm(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() # パラメータを更新\n",
    "\n",
    "        total_loss += loss.data \n",
    "        outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "        labels = labels.data.numpy()\n",
    "        hidden = hidden.detach()\n",
    "        for output, label in zip(outputs, labels):\n",
    "            if output == label:\n",
    "                n_correct += 1\n",
    "        \n",
    "    print('epoch: %d loss: %f accuracy: %f' % (epoch, loss, n_correct/dataset_size))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "'''\n",
    "\n",
    "100%|██████████| 11/11 [00:07<00:00,  1.53it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 0 loss: 1.363497 accuracy: 0.262650\n",
    "100%|██████████| 11/11 [00:07<00:00,  1.57it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 1 loss: 1.324291 accuracy: 0.397395\n",
    "100%|██████████| 11/11 [00:07<00:00,  1.44it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 2 loss: 1.300277 accuracy: 0.397395\n",
    "100%|██████████| 11/11 [00:07<00:00,  1.41it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 3 loss: 1.280347 accuracy: 0.397395\n",
    "100%|██████████| 11/11 [00:07<00:00,  1.56it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 4 loss: 1.279698 accuracy: 0.403298\n",
    "100%|██████████| 11/11 [00:07<00:00,  1.55it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 5 loss: 1.278832 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:07<00:00,  1.50it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 6 loss: 1.287330 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:07<00:00,  1.53it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 7 loss: 1.281508 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:07<00:00,  1.55it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 8 loss: 1.274512 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:07<00:00,  1.38it/s]epoch: 9 loss: 1.271275 accuracy: 0.418572\n",
    "Finished Training\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 84. 単語ベクトルの導入\n",
    "事前学習済みの単語ベクトル（例えば，Google Newsデータセット（約1,000億単語）での学習済み単語ベクトル）で単語埋め込みemb(x)を初期化し，学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "# googlenews = KeyedVectors.load_word2vec_format(\n",
    "#     '../../data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, output_size, vocab_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(dw, dh, nonlinearity='relu')\n",
    "        self.liner = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, data, last_hidden):           # data: (max_len, dw)\n",
    "        y, hidden = self.rnn(data, last_hidden)     # y: (max_len, dh), hidden: (max_len, dh)\n",
    "        y = y[:,-1,:]\n",
    "        y = self.liner(y)\n",
    "        y = torch.softmax(y, dim=1)\n",
    "        return y, hidden\n",
    "\n",
    "\n",
    "def tokens2vec(tokens, max_len):\n",
    "    vec = []\n",
    "    for token in tokens:\n",
    "        if token in googlenews:\n",
    "            vec.append(googlenews[token])\n",
    "        else:\n",
    "            vec.append([0]*dw)\n",
    "            \n",
    "    # padding\n",
    "    zeros = [0]*dw\n",
    "    vec += [zeros for _ in range(max_len-len(vec))]\n",
    "    return np.array(vec)\n",
    "            \n",
    "dataset_size = len(train)\n",
    "max_len = train.tokens.apply(len).max()\n",
    "\n",
    "X_train = train.tokens.progress_apply(tokens2vec, max_len=max_len).values.tolist()\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "max_len = len(X_train[0])\n",
    "\n",
    "Y_train = torch.tensor(Y_train).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNN(dw, dh, L, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=1024, shuffle=True)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    hidden = torch.zeros(1, max_len, dh, dtype=torch.float32)\n",
    "    n_correct = 0\n",
    "    total_loss = 0\n",
    "    for inputs, labels in tqdm(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() # パラメータを更新\n",
    "\n",
    "        total_loss += loss.data \n",
    "        outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "        labels = labels.data.numpy()\n",
    "        hidden = hidden.detach()\n",
    "        for output, label in zip(outputs, labels):\n",
    "            if output == label:\n",
    "                n_correct += 1\n",
    "        \n",
    "    print('epoch: %d loss: %f accuracy: %f' % (epoch, loss, n_correct/dataset_size))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "'''\n",
    "\n",
    "100%|██████████| 11/11 [00:04<00:00,  2.51it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 0 loss: 1.386395 accuracy: 0.114318\n",
    "100%|██████████| 11/11 [00:03<00:00,  2.80it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 1 loss: 1.384714 accuracy: 0.418478\n",
    "100%|██████████| 11/11 [00:03<00:00,  2.88it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 2 loss: 1.383234 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:03<00:00,  2.89it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 3 loss: 1.381624 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:03<00:00,  2.89it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 4 loss: 1.380694 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:04<00:00,  2.68it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 5 loss: 1.378607 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:04<00:00,  2.38it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 6 loss: 1.377145 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:04<00:00,  2.47it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 7 loss: 1.376932 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:04<00:00,  2.55it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 8 loss: 1.373910 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:04<00:00,  2.60it/s]epoch: 9 loss: 1.374367 accuracy: 0.418572\n",
    "Finished Training\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 85. 双方向RNN・多層化\n",
    "順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし，モデルを学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalRNN(nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, output_size, vocab_size):\n",
    "        super(BidirectionalRNN, self).__init__()\n",
    "        self.emb = torch.nn.Embedding(vocab_size, data_size)\n",
    "        self.rnn1 = torch.nn.RNN(data_size, hidden_size, nonlinearity='relu', bidirectional=True)\n",
    "        self.rnn2 = torch.nn.RNN(2*hidden_size, hidden_size, nonlinearity='relu', bidirectional=True)\n",
    "        self.liner = nn.Linear(2*hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, data, last_hidden):           # data: (max_len)\n",
    "        data = self.emb(data)                   # data: (max_length, dw)\n",
    "        y, hidden = self.rnn1(data, last_hidden)     # y: (max_len, dh), hidden: (max_len, dh)\n",
    "        y, hidden = self.rnn2(y, hidden)\n",
    "        y = y[:,-1,:]\n",
    "        y = self.liner(y)\n",
    "        y = torch.softmax(y, dim=1)\n",
    "        return y, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BidirectionalRNN(dw, dh, L, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "X_train = pad_sequence(X_train, batch_first=True)\n",
    "max_len = len(X_train[0])\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=1024, shuffle=True)\n",
    "\n",
    "model   = model.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    hidden = torch.zeros(2, max_len, dh, dtype=torch.float32)\n",
    "    n_correct = 0\n",
    "    total_loss = 0\n",
    "    for inputs, labels in tqdm(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() # パラメータを更新\n",
    "\n",
    "        total_loss += loss.data \n",
    "        outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "        labels = labels.data.numpy()\n",
    "        hidden = hidden.detach()\n",
    "        for output, label in zip(outputs, labels):\n",
    "            if output == label:\n",
    "                n_correct += 1\n",
    "        \n",
    "    print('epoch: %d loss: %f accuracy: %f' % (epoch, loss, n_correct/dataset_size))\n",
    "\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 86. 畳み込みニューラルネットワーク (CNN)\n",
    "ID番号で表現された単語列x=(x1,x2,…,xT)がある．ただし，Tは単語列の長さ，xt∈ℝVは単語のID番号のone-hot表記である（Vは単語の総数である）．畳み込みニューラルネットワーク（CNN: Convolutional Neural Network）を用い，単語列xからカテゴリyを予測するモデルを実装せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "dw = 300\n",
    "dh = 50\n",
    "L = 4\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, output_size, vocab_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.emb = torch.nn.Embedding(vocab_size, data_size)\n",
    "        self.conv = torch.nn.Conv1d(data_size, hidden_size, 3, padding=1) # in_channels, out_channels, kernel_sizes\n",
    "        self.pool = torch.nn.MaxPool1d(120)\n",
    "        self.liner_px = nn.Linear(data_size*3, hidden_size)\n",
    "        self.liner_yc = nn.Linear(hidden_size, output_size)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):                       # x: (max_len)\n",
    "        x = self.emb(x)                         # x: (max_length, dw)\n",
    "        x = x.view(-1, x.shape[2], x.shape[1])  # x: (dw, max_length)\n",
    "        x = self.conv(x)                        # 畳み込み x: (dh, max_len)\n",
    "        p = self.act(x)\n",
    "        c = self.pool(p)                        # c: (dh, 1)\n",
    "        c = c.view(c.shape[0], c.shape[1])      # c: (1, dh)\n",
    "        y = self.liner_yc(c)                    # c: (1, L)\n",
    "        y = torch.softmax(y, dim=1)\n",
    "        return y\n",
    "\n",
    "\n",
    "X_train = train.tokens.apply(tokens2ids)\n",
    "max_len = train.tokens.apply(len).max()\n",
    "model = CNN(dw, dh, L, vocab_size)\n",
    "\n",
    "inputs = pad_sequence(X_train, batch_first=True)\n",
    "\n",
    "outputs = model(inputs[:1])\n",
    "print('output.size', outputs.size())\n",
    "print(outputs)\n",
    "\n",
    "'''\n",
    "output.size torch.Size([1, 4])\n",
    "tensor([[0.1083, 0.2877, 0.4019, 0.2021]], grad_fn=<SoftmaxBackward>)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 87. 確率的勾配降下法によるCNNの学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題86で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ('category', 'title')\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "test = pd.read_csv('../../data/NewsAggregatorDataset/test.txt',\n",
    "                   names=columns, sep='\\t')\n",
    "\n",
    "train['tokens'] = train.title.apply(preprocessor)\n",
    "test['tokens'] = test.title.apply(preprocessor)\n",
    "\n",
    "X_train = train.tokens.apply(tokens2ids)\n",
    "X_train = pad_sequence(X_train, batch_first=True)\n",
    "X_test = test.tokens.apply(tokens2ids)\n",
    "X_test = pad_sequence(X_test, batch_first=True)\n",
    "\n",
    "label2int = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "Y_train = train.category.map(label2int)\n",
    "Y_test = test.category.map(label2int)\n",
    "Y_train = torch.tensor(Y_train).long()\n",
    "Y_test = torch.tensor(Y_test).long()\n",
    "\n",
    "max_len = train.tokens.apply(len).max()\n",
    "dataset_size = len(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ('category', 'title')\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "test = pd.read_csv('../../data/NewsAggregatorDataset/test.txt',\n",
    "                   names=columns, sep='\\t')\n",
    "\n",
    "train['tokens'] = train.title.apply(preprocessor)\n",
    "test['tokens'] = test.title.apply(preprocessor)\n",
    "\n",
    "X_train = train.tokens.apply(tokens2ids)\n",
    "X_train = pad_sequence(X_train, batch_first=True)\n",
    "X_test = test.tokens.apply(tokens2ids)\n",
    "X_test = pad_sequence(X_test, batch_first=True)\n",
    "\n",
    "label2int = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "Y_train = train.category.map(label2int)\n",
    "Y_test = test.category.map(label2int)\n",
    "Y_train = torch.tensor(Y_train).long()\n",
    "Y_test = torch.tensor(Y_test).long()\n",
    "\n",
    "max_len = train.tokens.apply(len).max()\n",
    "dataset_size = len(train)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN(dw, dh, L, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=1024, shuffle=True)\n",
    "\n",
    "model   = model.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    n_correct = 0\n",
    "    total_loss = 0\n",
    "    for inputs, labels in tqdm(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() # パラメータを更新\n",
    "        \n",
    "        total_loss += loss.data \n",
    "        outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "        labels = labels.data.numpy()\n",
    "        for output, label in zip(outputs, labels):\n",
    "            if output == label:\n",
    "                n_correct += 1\n",
    "        \n",
    "    print('epoch: %d loss: %f accuracy: %f' % (epoch, loss, n_correct/dataset_size))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "'''\n",
    "100%|██████████| 11/11 [00:14<00:00,  1.36s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 0 loss: 1.324250 accuracy: 0.366567\n",
    "100%|██████████| 11/11 [00:15<00:00,  1.42s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 1 loss: 1.291704 accuracy: 0.397395\n",
    "100%|██████████| 11/11 [00:18<00:00,  1.69s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 2 loss: 1.272223 accuracy: 0.456241\n",
    "100%|██████████| 11/11 [00:16<00:00,  1.49s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 3 loss: 1.263339 accuracy: 0.484726\n",
    "100%|██████████| 11/11 [00:23<00:00,  2.10s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 4 loss: 1.244924 accuracy: 0.500281\n",
    "100%|██████████| 11/11 [00:16<00:00,  1.52s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 5 loss: 1.251264 accuracy: 0.479854\n",
    "100%|██████████| 11/11 [00:14<00:00,  1.35s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 6 loss: 1.244529 accuracy: 0.523145\n",
    "100%|██████████| 11/11 [00:15<00:00,  1.39s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 7 loss: 1.243323 accuracy: 0.515555\n",
    "100%|██████████| 11/11 [00:21<00:00,  1.98s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 8 loss: 1.242815 accuracy: 0.535139\n",
    "100%|██████████| 11/11 [00:14<00:00,  1.36s/it]epoch: 9 loss: 1.226233 accuracy: 0.531297\n",
    "Finished Training\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 88. パラメータチューニング\n",
    "問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 89. 事前学習済み言語モデルからの転移学習\n",
    "事前学習済み言語モデル（例えばBERTなど）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "class Bert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.bert = \n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.bert(data)\n",
    "        return x\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
    "\n",
    "# inputs = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "inputs = X_train[0].unsqueeze(0)\n",
    "# labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "labels = Y_train[0]\n",
    "outputs = model(inputs, labels=labels)\n",
    "\n",
    "loss = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(0.6970, grad_fn=<NllLossBackward>)"
     },
     "metadata": {},
     "execution_count": 103
    }
   ],
   "source": [
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}