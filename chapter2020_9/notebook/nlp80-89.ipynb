{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 80. ID番号への変換\n",
    "問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に1，2番目に頻出する単語に2，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて0とせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def tokenize(doc):\n",
    "    tokens = doc.split(' ')\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def normalize(doc):\n",
    "    doc = re.sub(r\"[',.]\", '', doc)   # 記号を削除\n",
    "    doc = re.sub(r\" {2,}\", ' ', doc)  # 2回以上続くスペースを削除\n",
    "    doc = re.sub(r\" *?$\", '', doc)    # 行頭と行末のスペースを削除\n",
    "    doc = re.sub(r\"^ *?\", '', doc)\n",
    "    doc = doc.lower()                 # 小文字に統一\n",
    "    return doc\n",
    "\n",
    "\n",
    "def token2id(token):\n",
    "    if token in token2id_dic:\n",
    "        return token2id_dic[token]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "columns = ('category', 'title')\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "\n",
    "\n",
    "docs = [normalize(doc) for doc in train.title.values.tolist()]\n",
    "tokens = [tokenize(doc) for doc in docs]\n",
    "tokens = sum(tokens, [])  # flat list\n",
    "counter = Counter(tokens)\n",
    "\n",
    "token2id_dic = {}\n",
    "vocab_size = len(counter)\n",
    "for index, (token, freq) in enumerate(counter.most_common(), 1):\n",
    "    if freq < 2:\n",
    "        token2id_dic[token] = 0\n",
    "    else:\n",
    "        token2id_dic[token] = index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id('the')\n",
    "'''\n",
    "3\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 81. RNNによる予測\n",
    "ID番号で表現された単語列x=(x1,x2,…,xT)がある．ただし，Tは単語列の長さ，xt∈ℝVは単語のID番号のone-hot表記である（Vは単語の総数である）．再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い，単語列xからカテゴリyを予測するモデルとして，次式を実装せよ...\n",
    "[参考](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "def preprocessor(doc):\n",
    "    doc = normalize(doc)    \n",
    "    tokens = tokenize(doc)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokens2ids(tokens):\n",
    "    tokens = [token2id(token) for token in tokens]\n",
    "    return torch.tensor(tokens, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = 300\n",
    "dh = 50\n",
    "L = 4\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, output_size, vocab_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.emb = torch.nn.Embedding(vocab_size, data_size)\n",
    "        self.rnn = torch.nn.RNN(dw, dh, nonlinearity='relu')\n",
    "        self.liner = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, data, last_hidden):           # data: (max_len)\n",
    "        data = self.emb(data)                       # data: (max_length, dw)\n",
    "        y, hidden = self.rnn(data, last_hidden)     # y: (max_len, dh), hidden: (max_len, dh)\n",
    "        y = y[:,-1,:]\n",
    "        y = self.liner(y)\n",
    "        y = torch.softmax(y, dim=1)\n",
    "        return y, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tokens'] = train.title.apply(preprocessor)\n",
    "X_train = train.tokens.apply(tokens2ids)\n",
    "X_train[0]\n",
    "\n",
    "# tensor([   8,    0, 2416, 1604, 2143,    5, 1605,    4,  745])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = train.tokens.apply(len).max()\n",
    "model = RNN(dw, dh, L, vocab_size)\n",
    "\n",
    "inputs = pad_sequence(X_train, batch_first=True)\n",
    "max_len = len(inputs[0])\n",
    "h0 = torch.zeros(1, max_len, dh, dtype=torch.float32)\n",
    "\n",
    "outputs, hidden = model(inputs, h0)\n",
    "print(outputs.size())\n",
    "print(hidden.size())\n",
    "\n",
    "'''\n",
    "torch.Size([10672, 4])\n",
    "torch.Size([1, 121, 50])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 82. 確率的勾配降下法による学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題81で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ('category', 'title')\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "test = pd.read_csv('../../data/NewsAggregatorDataset/test.txt',\n",
    "                   names=columns, sep='\\t')\n",
    "\n",
    "train['tokens'] = train.title.apply(preprocessor)\n",
    "test['tokens'] = test.title.apply(preprocessor)\n",
    "\n",
    "X_train = train.tokens.apply(tokens2ids)\n",
    "X_test = test.tokens.apply(tokens2ids)\n",
    "\n",
    "label2int = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "Y_train = train.category.map(label2int)\n",
    "Y_test = test.category.map(label2int)\n",
    "Y_train = torch.tensor(Y_train).long()\n",
    "Y_test = torch.tensor(Y_test).long()\n",
    "\n",
    "dataset_size = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "def accuracy(pred, label):\n",
    "    pred = np.argmax(pred.data.numpy(), axis=1) # 行ごとに最大値のインデックスを取得する．\n",
    "    label = label.data.numpy()\n",
    "    return (pred == label).mean()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNN(dw, dh, L, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "X_train = pad_sequence(X_train, batch_first=True)\n",
    "max_len = len(X_train[0])\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=1, shuffle=True)\n",
    "\n",
    "model   = model.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    hidden = torch.zeros(1, max_len, dh, dtype=torch.float32)\n",
    "    n_correct = 0\n",
    "    total_loss = 0\n",
    "    for inputs, label in tqdm(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        label = label.to(device)\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(outputs, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() # パラメータを更新\n",
    "\n",
    "        total_loss += loss.data \n",
    "        outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "        label = label.data.numpy()\n",
    "        hidden = hidden.detach()\n",
    "        if outputs == label:\n",
    "            n_correct += 1\n",
    "        \n",
    "    print('epoch: %d loss: %f accuracy: %f' % (epoch, loss, n_correct/dataset_size))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "'''\n",
    "100%|██████████| 10672/10672 [02:33<00:00, 69.55it/s]\n",
    "  0%|          | 9/10672 [00:00<02:10, 81.85it/s]epoch: 0 loss: 1.295777 accuracy: 0.416698\n",
    "100%|██████████| 10672/10672 [02:31<00:00, 70.39it/s]\n",
    "  0%|          | 7/10672 [00:00<02:38, 67.40it/s]epoch: 1 loss: 1.220215 accuracy: 0.407234\n",
    "100%|██████████| 10672/10672 [02:17<00:00, 77.80it/s]\n",
    "  0%|          | 8/10672 [00:00<02:21, 75.47it/s]epoch: 2 loss: 1.665452 accuracy: 0.411544\n",
    "100%|██████████| 10672/10672 [02:12<00:00, 80.81it/s]\n",
    "  0%|          | 8/10672 [00:00<02:21, 75.41it/s]epoch: 3 loss: 1.081051 accuracy: 0.410795\n",
    "100%|██████████| 10672/10672 [02:38<00:00, 67.43it/s]\n",
    "  0%|          | 8/10672 [00:00<02:18, 77.02it/s]epoch: 4 loss: 1.035693 accuracy: 0.412950\n",
    "100%|██████████| 10672/10672 [02:38<00:00, 67.30it/s]\n",
    "  0%|          | 3/10672 [00:00<06:55, 25.66it/s]epoch: 5 loss: 1.218706 accuracy: 0.416229\n",
    "100%|██████████| 10672/10672 [02:19<00:00, 76.62it/s]\n",
    "  0%|          | 7/10672 [00:00<02:37, 67.82it/s]epoch: 6 loss: 1.135605 accuracy: 0.408452\n",
    "100%|██████████| 10672/10672 [02:02<00:00, 86.95it/s]\n",
    "  0%|          | 9/10672 [00:00<02:08, 82.73it/s]epoch: 7 loss: 1.205219 accuracy: 0.413137\n",
    "100%|██████████| 10672/10672 [02:12<00:00, 80.55it/s]\n",
    "  0%|          | 8/10672 [00:00<02:21, 75.44it/s]epoch: 8 loss: 1.667904 accuracy: 0.414543\n",
    "100%|██████████| 10672/10672 [02:12<00:00, 80.39it/s]epoch: 9 loss: 1.123815 accuracy: 0.413981\n",
    "Finished Training\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 83. ミニバッチ化・GPU上での学習\n",
    "問題82のコードを改変し，B事例ごとに損失・勾配を計算して学習を行えるようにせよ（Bの値は適当に選べ）．また，GPU上で学習を実行せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNN(dw, dh, L, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "X_train = pad_sequence(X_train, batch_first=True)\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=1024, shuffle=True)\n",
    "\n",
    "model   = model.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    hidden = torch.zeros(1, max_len, dh, dtype=torch.float32)\n",
    "    n_correct = 0\n",
    "    total_loss = 0\n",
    "    for inputs, labels in tqdm(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() # パラメータを更新\n",
    "\n",
    "        total_loss += loss.data \n",
    "        outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "        labels = labels.data.numpy()\n",
    "        hidden = hidden.detach()\n",
    "        for output, label in zip(outputs, labels):\n",
    "            if output == label:\n",
    "                n_correct += 1\n",
    "        \n",
    "    print('epoch: %d loss: %f accuracy: %f' % (epoch, loss, n_correct/dataset_size))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "'''\n",
    "\n",
    "100%|██████████| 11/11 [00:07<00:00,  1.53it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 0 loss: 1.363497 accuracy: 0.262650\n",
    "100%|██████████| 11/11 [00:07<00:00,  1.57it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 1 loss: 1.324291 accuracy: 0.397395\n",
    "100%|██████████| 11/11 [00:07<00:00,  1.44it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 2 loss: 1.300277 accuracy: 0.397395\n",
    "100%|██████████| 11/11 [00:07<00:00,  1.41it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 3 loss: 1.280347 accuracy: 0.397395\n",
    "100%|██████████| 11/11 [00:07<00:00,  1.56it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 4 loss: 1.279698 accuracy: 0.403298\n",
    "100%|██████████| 11/11 [00:07<00:00,  1.55it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 5 loss: 1.278832 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:07<00:00,  1.50it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 6 loss: 1.287330 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:07<00:00,  1.53it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 7 loss: 1.281508 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:07<00:00,  1.55it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 8 loss: 1.274512 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:07<00:00,  1.38it/s]epoch: 9 loss: 1.271275 accuracy: 0.418572\n",
    "Finished Training\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 84. 単語ベクトルの導入\n",
    "事前学習済みの単語ベクトル（例えば，Google Newsデータセット（約1,000億単語）での学習済み単語ベクトル）で単語埋め込みemb(x)を初期化し，学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "# googlenews = KeyedVectors.load_word2vec_format(\n",
    "#     '../../data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, output_size, vocab_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(dw, dh, nonlinearity='relu')\n",
    "        self.liner = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, data, last_hidden):           # data: (max_len, dw)\n",
    "        y, hidden = self.rnn(data, last_hidden)     # y: (max_len, dh), hidden: (max_len, dh)\n",
    "        y = y[:,-1,:]\n",
    "        y = self.liner(y)\n",
    "        y = torch.softmax(y, dim=1)\n",
    "        return y, hidden\n",
    "\n",
    "\n",
    "def tokens2vec(tokens, max_len):\n",
    "    vec = []\n",
    "    for token in tokens:\n",
    "        if token in googlenews:\n",
    "            vec.append(googlenews[token])\n",
    "        else:\n",
    "            vec.append([0]*dw)\n",
    "            \n",
    "    # padding\n",
    "    zeros = [0]*dw\n",
    "    vec += [zeros for _ in range(max_len-len(vec))]\n",
    "    return np.array(vec)\n",
    "            \n",
    "dataset_size = len(train)\n",
    "max_len = train.tokens.apply(len).max()\n",
    "\n",
    "X_train = train.tokens.progress_apply(tokens2vec, max_len=max_len).values.tolist()\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "max_len = len(X_train[0])\n",
    "\n",
    "Y_train = torch.tensor(Y_train).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNN(dw, dh, L, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=1024, shuffle=True)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    hidden = torch.zeros(1, max_len, dh, dtype=torch.float32)\n",
    "    n_correct = 0\n",
    "    total_loss = 0\n",
    "    for inputs, labels in tqdm(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() # パラメータを更新\n",
    "\n",
    "        total_loss += loss.data \n",
    "        outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "        labels = labels.data.numpy()\n",
    "        hidden = hidden.detach()\n",
    "        for output, label in zip(outputs, labels):\n",
    "            if output == label:\n",
    "                n_correct += 1\n",
    "        \n",
    "    print('epoch: %d loss: %f accuracy: %f' % (epoch, loss, n_correct/dataset_size))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "'''\n",
    "\n",
    "100%|██████████| 11/11 [00:04<00:00,  2.51it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 0 loss: 1.386395 accuracy: 0.114318\n",
    "100%|██████████| 11/11 [00:03<00:00,  2.80it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 1 loss: 1.384714 accuracy: 0.418478\n",
    "100%|██████████| 11/11 [00:03<00:00,  2.88it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 2 loss: 1.383234 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:03<00:00,  2.89it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 3 loss: 1.381624 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:03<00:00,  2.89it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 4 loss: 1.380694 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:04<00:00,  2.68it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 5 loss: 1.378607 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:04<00:00,  2.38it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 6 loss: 1.377145 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:04<00:00,  2.47it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 7 loss: 1.376932 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:04<00:00,  2.55it/s]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 8 loss: 1.373910 accuracy: 0.418572\n",
    "100%|██████████| 11/11 [00:04<00:00,  2.60it/s]epoch: 9 loss: 1.374367 accuracy: 0.418572\n",
    "Finished Training\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 85. 双方向RNN・多層化\n",
    "順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし，モデルを学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalRNN(nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, output_size, vocab_size):\n",
    "        super(BidirectionalRNN, self).__init__()\n",
    "        self.emb = torch.nn.Embedding(vocab_size, data_size)\n",
    "        self.rnn1 = torch.nn.RNN(data_size, hidden_size, nonlinearity='relu', bidirectional=True)\n",
    "        self.rnn2 = torch.nn.RNN(2*hidden_size, hidden_size, nonlinearity='relu', bidirectional=True)\n",
    "        self.liner = nn.Linear(2*hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, data, last_hidden):           # data: (max_len)\n",
    "        data = self.emb(data)                   # data: (max_length, dw)\n",
    "        y, hidden = self.rnn1(data, last_hidden)     # y: (max_len, dh), hidden: (max_len, dh)\n",
    "        y, hidden = self.rnn2(y, hidden)\n",
    "        y = y[:,-1,:]\n",
    "        y = self.liner(y)\n",
    "        y = torch.softmax(y, dim=1)\n",
    "        return y, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BidirectionalRNN(dw, dh, L, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "X_train = pad_sequence(X_train, batch_first=True)\n",
    "max_len = len(X_train[0])\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=1024, shuffle=True)\n",
    "\n",
    "model   = model.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    hidden = torch.zeros(2, max_len, dh, dtype=torch.float32)\n",
    "    n_correct = 0\n",
    "    total_loss = 0\n",
    "    for inputs, labels in tqdm(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() # パラメータを更新\n",
    "\n",
    "        total_loss += loss.data \n",
    "        outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "        labels = labels.data.numpy()\n",
    "        hidden = hidden.detach()\n",
    "        for output, label in zip(outputs, labels):\n",
    "            if output == label:\n",
    "                n_correct += 1\n",
    "        \n",
    "    print('epoch: %d loss: %f accuracy: %f' % (epoch, loss, n_correct/dataset_size))\n",
    "\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 86. 畳み込みニューラルネットワーク (CNN)\n",
    "ID番号で表現された単語列x=(x1,x2,…,xT)がある．ただし，Tは単語列の長さ，xt∈ℝVは単語のID番号のone-hot表記である（Vは単語の総数である）．畳み込みニューラルネットワーク（CNN: Convolutional Neural Network）を用い，単語列xからカテゴリyを予測するモデルを実装せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "dw = 300\n",
    "dh = 50\n",
    "L = 4\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, output_size, vocab_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.emb = torch.nn.Embedding(vocab_size, data_size)\n",
    "        self.conv = torch.nn.Conv1d(data_size, hidden_size, 3, padding=1) # in_channels, out_channels, kernel_sizes\n",
    "        self.pool = torch.nn.MaxPool1d(120)\n",
    "        self.liner_px = nn.Linear(data_size*3, hidden_size)\n",
    "        self.liner_yc = nn.Linear(hidden_size, output_size)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):                       # x: (max_len)\n",
    "        x = self.emb(x)                         # x: (max_length, dw)\n",
    "        x = x.view(-1, x.shape[2], x.shape[1])  # x: (dw, max_length)\n",
    "        x = self.conv(x)                        # 畳み込み x: (dh, max_len)\n",
    "        p = self.act(x)\n",
    "        c = self.pool(p)                        # c: (dh, 1)\n",
    "        c = c.view(c.shape[0], c.shape[1])      # c: (1, dh)\n",
    "        y = self.liner_yc(c)                    # c: (1, L)\n",
    "        y = torch.softmax(y, dim=1)\n",
    "        return y\n",
    "\n",
    "\n",
    "X_train = train.tokens.apply(tokens2ids)\n",
    "max_len = train.tokens.apply(len).max()\n",
    "model = CNN(dw, dh, L, vocab_size)\n",
    "\n",
    "inputs = pad_sequence(X_train, batch_first=True)\n",
    "\n",
    "outputs = model(inputs[:1])\n",
    "print('output.size', outputs.size())\n",
    "print(outputs)\n",
    "\n",
    "'''\n",
    "output.size torch.Size([1, 4])\n",
    "tensor([[0.1083, 0.2877, 0.4019, 0.2021]], grad_fn=<SoftmaxBackward>)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 87. 確率的勾配降下法によるCNNの学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題86で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ('category', 'title')\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "test = pd.read_csv('../../data/NewsAggregatorDataset/test.txt',\n",
    "                   names=columns, sep='\\t')\n",
    "\n",
    "train['tokens'] = train.title.apply(preprocessor)\n",
    "test['tokens'] = test.title.apply(preprocessor)\n",
    "\n",
    "X_train = train.tokens.apply(tokens2ids)\n",
    "X_train = pad_sequence(X_train, batch_first=True)\n",
    "X_test = test.tokens.apply(tokens2ids)\n",
    "X_test = pad_sequence(X_test, batch_first=True)\n",
    "\n",
    "label2int = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "Y_train = train.category.map(label2int)\n",
    "Y_test = test.category.map(label2int)\n",
    "Y_train = torch.tensor(Y_train).long()\n",
    "Y_test = torch.tensor(Y_test).long()\n",
    "\n",
    "max_len = train.tokens.apply(len).max()\n",
    "dataset_size = len(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ('category', 'title')\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "test = pd.read_csv('../../data/NewsAggregatorDataset/test.txt',\n",
    "                   names=columns, sep='\\t')\n",
    "\n",
    "train['tokens'] = train.title.apply(preprocessor)\n",
    "test['tokens'] = test.title.apply(preprocessor)\n",
    "\n",
    "X_train = train.tokens.apply(tokens2ids)\n",
    "X_train = pad_sequence(X_train, batch_first=True)\n",
    "X_test = test.tokens.apply(tokens2ids)\n",
    "X_test = pad_sequence(X_test, batch_first=True)\n",
    "\n",
    "label2int = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "Y_train = train.category.map(label2int)\n",
    "Y_test = test.category.map(label2int)\n",
    "Y_train = torch.tensor(Y_train).long()\n",
    "Y_test = torch.tensor(Y_test).long()\n",
    "\n",
    "max_len = train.tokens.apply(len).max()\n",
    "dataset_size = len(train)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN(dw, dh, L, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=1024, shuffle=True)\n",
    "\n",
    "model   = model.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    n_correct = 0\n",
    "    total_loss = 0\n",
    "    for inputs, labels in tqdm(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() # パラメータを更新\n",
    "        \n",
    "        total_loss += loss.data \n",
    "        outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "        labels = labels.data.numpy()\n",
    "        for output, label in zip(outputs, labels):\n",
    "            if output == label:\n",
    "                n_correct += 1\n",
    "        \n",
    "    print('epoch: %d loss: %f accuracy: %f' % (epoch, loss, n_correct/dataset_size))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "'''\n",
    "100%|██████████| 11/11 [00:14<00:00,  1.36s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 0 loss: 1.324250 accuracy: 0.366567\n",
    "100%|██████████| 11/11 [00:15<00:00,  1.42s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 1 loss: 1.291704 accuracy: 0.397395\n",
    "100%|██████████| 11/11 [00:18<00:00,  1.69s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 2 loss: 1.272223 accuracy: 0.456241\n",
    "100%|██████████| 11/11 [00:16<00:00,  1.49s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 3 loss: 1.263339 accuracy: 0.484726\n",
    "100%|██████████| 11/11 [00:23<00:00,  2.10s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 4 loss: 1.244924 accuracy: 0.500281\n",
    "100%|██████████| 11/11 [00:16<00:00,  1.52s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 5 loss: 1.251264 accuracy: 0.479854\n",
    "100%|██████████| 11/11 [00:14<00:00,  1.35s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 6 loss: 1.244529 accuracy: 0.523145\n",
    "100%|██████████| 11/11 [00:15<00:00,  1.39s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 7 loss: 1.243323 accuracy: 0.515555\n",
    "100%|██████████| 11/11 [00:21<00:00,  1.98s/it]\n",
    "  0%|          | 0/11 [00:00<?, ?it/s]epoch: 8 loss: 1.242815 accuracy: 0.535139\n",
    "100%|██████████| 11/11 [00:14<00:00,  1.36s/it]epoch: 9 loss: 1.226233 accuracy: 0.531297\n",
    "Finished Training\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 88. パラメータチューニング\n",
    "問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 89. 事前学習済み言語モデルからの転移学習\n",
    "事前学習済み言語モデル（例えばBERTなど）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "class Bert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.bert = \n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.bert(data)\n",
    "        return x\n",
    "\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# inputs = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "inputs = X_train[0].unsqueeze(0)\n",
    "# labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "labels = Y_train[0]\n",
    "outputs = model(inputs, labels=labels)\n",
    "\n",
    "loss = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(0.6970, grad_fn=<NllLossBackward>)"
     },
     "metadata": {},
     "execution_count": 103
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([1, 121])"
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}