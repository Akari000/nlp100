{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 80. ID番号への変換\n",
    "問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に1，2番目に頻出する単語に2，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて0とせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def tokenize(doc):\n",
    "    tokens = doc.split(' ')\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def normalize(doc):\n",
    "    doc = re.sub(r\"[',.]\", '', doc)   # 記号を削除\n",
    "    doc = re.sub(r\" {2,}\", ' ', doc)  # 2回以上続くスペースを削除\n",
    "    doc = re.sub(r\" *?$\", '', doc)    # 行頭と行末のスペースを削除\n",
    "    doc = re.sub(r\"^ *?\", '', doc)\n",
    "    doc = doc.lower()                 # 小文字に統一\n",
    "    return doc\n",
    "\n",
    "\n",
    "def token2id(token):\n",
    "    if token in token2id_dic:\n",
    "        return token2id_dic[token]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "columns = ('category', 'title')\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "\n",
    "\n",
    "docs = [normalize(doc) for doc in train.title.values.tolist()]\n",
    "tokens = [tokenize(doc) for doc in docs]\n",
    "tokens = sum(tokens, [])  # flat list\n",
    "counter = Counter(tokens)\n",
    "\n",
    "token2id_dic = {}\n",
    "vocab_size = len(counter)\n",
    "for index, (token, freq) in enumerate(counter.most_common(), 1):\n",
    "    if freq < 2:\n",
    "        token2id_dic[token] = 0\n",
    "    else:\n",
    "        token2id_dic[token] = index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3"
     },
     "metadata": {},
     "execution_count": 272
    }
   ],
   "source": [
    "token2id('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 81. RNNによる予測\n",
    "ID番号で表現された単語列x=(x1,x2,…,xT)がある．ただし，Tは単語列の長さ，xt∈ℝVは単語のID番号のone-hot表記である（Vは単語の総数である）．再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い，単語列xからカテゴリyを予測するモデルとして，次式を実装せよ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "def preprocessor(doc):\n",
    "    doc = normalize(doc)    \n",
    "    tokens = tokenize(doc)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokens2ids(tokens):\n",
    "    tokens = [token2id(token) for token in tokens]\n",
    "    return torch.tensor(tokens, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = 300\n",
    "dh = 50\n",
    "L = 4\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, output_size, vocab_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.emb = torch.nn.Embedding(vocab_size, data_size)\n",
    "        self.rnn = torch.nn.RNN(dw, dh, nonlinearity='relu')\n",
    "        self.liner = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, data, last_hidden):           # data: (max_len)\n",
    "        data = self.emb(data)                       # data: (max_length, dw)\n",
    "        y, hidden = self.rnn(data, last_hidden)     # y: (max_len, dh), hidden: (max_len, dh)\n",
    "        y = y[:,-1,:]\n",
    "        y = self.liner(y)\n",
    "        y = torch.softmax(y, dim=1)\n",
    "        return y, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([   8,    0, 2416, 1604, 2143,    5, 1605,    4,  745])"
     },
     "metadata": {},
     "execution_count": 274
    }
   ],
   "source": [
    "train['tokens'] = train.title.apply(preprocessor)\n",
    "X_train = train.tokens.apply(tokens2ids)\n",
    "X_train[0]\n",
    "\n",
    "# tensor([   8,    0, 2416, 1604, 2143,    5, 1605,    4,  745])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([10672, 4])\ntorch.Size([1, 121, 50])\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\ntorch.Size([10672, 4])\\ntorch.Size([1, 121, 50])\\n'"
     },
     "metadata": {},
     "execution_count": 275
    }
   ],
   "source": [
    "max_len = train.tokens.apply(len).max()\n",
    "model = RNN(dw, dh, L, vocab_size)\n",
    "\n",
    "inputs = pad_sequence(X_train, batch_first=True)\n",
    "h0 = torch.zeros(1, 121, dh, dtype=torch.float32)\n",
    "\n",
    "outputs, hidden = model(inputs, h0)\n",
    "print(outputs.size())\n",
    "print(hidden.size())\n",
    "\n",
    "'''\n",
    "torch.Size([10672, 4])\n",
    "torch.Size([1, 121, 50])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 82. 確率的勾配降下法による学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題81で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ('category', 'title')\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "test = pd.read_csv('../../data/NewsAggregatorDataset/test.txt',\n",
    "                   names=columns, sep='\\t')\n",
    "\n",
    "train['tokens'] = train.title.apply(preprocessor)\n",
    "test['tokens'] = test.title.apply(preprocessor)\n",
    "\n",
    "X_train = train.tokens.apply(tokens2ids)\n",
    "X_test = test.tokens.apply(tokens2ids)\n",
    "\n",
    "label2int = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "Y_train = train.category.map(label2int)\n",
    "Y_test = test.category.map(label2int)\n",
    "Y_train = torch.tensor(Y_train).long()\n",
    "Y_test = torch.tensor(Y_test).long()\n",
    "\n",
    "max_len = train.tokens.apply(len).max()\n",
    "dataset_size = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 10672/10672 [02:33<00:00, 69.55it/s]\n  0%|          | 9/10672 [00:00<02:10, 81.85it/s]epoch: 0 loss: 1.295777 accuracy: 0.416698\n100%|██████████| 10672/10672 [02:31<00:00, 70.39it/s]\n  0%|          | 7/10672 [00:00<02:38, 67.40it/s]epoch: 1 loss: 1.220215 accuracy: 0.407234\n100%|██████████| 10672/10672 [02:17<00:00, 77.80it/s]\n  0%|          | 8/10672 [00:00<02:21, 75.47it/s]epoch: 2 loss: 1.665452 accuracy: 0.411544\n100%|██████████| 10672/10672 [02:12<00:00, 80.81it/s]\n  0%|          | 8/10672 [00:00<02:21, 75.41it/s]epoch: 3 loss: 1.081051 accuracy: 0.410795\n100%|██████████| 10672/10672 [02:38<00:00, 67.43it/s]\n  0%|          | 8/10672 [00:00<02:18, 77.02it/s]epoch: 4 loss: 1.035693 accuracy: 0.412950\n100%|██████████| 10672/10672 [02:38<00:00, 67.30it/s]\n  0%|          | 3/10672 [00:00<06:55, 25.66it/s]epoch: 5 loss: 1.218706 accuracy: 0.416229\n100%|██████████| 10672/10672 [02:19<00:00, 76.62it/s]\n  0%|          | 7/10672 [00:00<02:37, 67.82it/s]epoch: 6 loss: 1.135605 accuracy: 0.408452\n100%|██████████| 10672/10672 [02:02<00:00, 86.95it/s]\n  0%|          | 9/10672 [00:00<02:08, 82.73it/s]epoch: 7 loss: 1.205219 accuracy: 0.413137\n100%|██████████| 10672/10672 [02:12<00:00, 80.55it/s]\n  0%|          | 8/10672 [00:00<02:21, 75.44it/s]epoch: 8 loss: 1.667904 accuracy: 0.414543\n100%|██████████| 10672/10672 [02:12<00:00, 80.39it/s]epoch: 9 loss: 1.123815 accuracy: 0.413981\nFinished Training\n\n"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "def accuracy(pred, label):\n",
    "    pred = np.argmax(pred.data.numpy(), axis=1) # 行ごとに最大値のインデックスを取得する．\n",
    "    label = label.data.numpy()\n",
    "    return (pred == label).mean()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNN(dw, dh, L, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "X_train = pad_sequence(X_train, batch_first=True)\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=1, shuffle=True)\n",
    "\n",
    "model   = model.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    hidden = torch.zeros(1, 121, dh, dtype=torch.float32)\n",
    "    n_correct = 0\n",
    "    total_loss = 0\n",
    "    for inputs, label in tqdm(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        label = label.to(device)\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(outputs, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() # パラメータを更新\n",
    "\n",
    "        total_loss += loss.data \n",
    "        outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "        label = label.data.numpy()\n",
    "        hidden = hidden.detach()\n",
    "        if outputs == label:\n",
    "            n_correct += 1\n",
    "        \n",
    "    print('epoch: %d loss: %f accuracy: %f' % (epoch, loss, n_correct/dataset_size))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "'''\n",
    "100%|██████████| 10672/10672 [02:33<00:00, 69.55it/s]\n",
    "  0%|          | 9/10672 [00:00<02:10, 81.85it/s]epoch: 0 loss: 1.295777 accuracy: 0.416698\n",
    "100%|██████████| 10672/10672 [02:31<00:00, 70.39it/s]\n",
    "  0%|          | 7/10672 [00:00<02:38, 67.40it/s]epoch: 1 loss: 1.220215 accuracy: 0.407234\n",
    "100%|██████████| 10672/10672 [02:17<00:00, 77.80it/s]\n",
    "  0%|          | 8/10672 [00:00<02:21, 75.47it/s]epoch: 2 loss: 1.665452 accuracy: 0.411544\n",
    "100%|██████████| 10672/10672 [02:12<00:00, 80.81it/s]\n",
    "  0%|          | 8/10672 [00:00<02:21, 75.41it/s]epoch: 3 loss: 1.081051 accuracy: 0.410795\n",
    "100%|██████████| 10672/10672 [02:38<00:00, 67.43it/s]\n",
    "  0%|          | 8/10672 [00:00<02:18, 77.02it/s]epoch: 4 loss: 1.035693 accuracy: 0.412950\n",
    "100%|██████████| 10672/10672 [02:38<00:00, 67.30it/s]\n",
    "  0%|          | 3/10672 [00:00<06:55, 25.66it/s]epoch: 5 loss: 1.218706 accuracy: 0.416229\n",
    "100%|██████████| 10672/10672 [02:19<00:00, 76.62it/s]\n",
    "  0%|          | 7/10672 [00:00<02:37, 67.82it/s]epoch: 6 loss: 1.135605 accuracy: 0.408452\n",
    "100%|██████████| 10672/10672 [02:02<00:00, 86.95it/s]\n",
    "  0%|          | 9/10672 [00:00<02:08, 82.73it/s]epoch: 7 loss: 1.205219 accuracy: 0.413137\n",
    "100%|██████████| 10672/10672 [02:12<00:00, 80.55it/s]\n",
    "  0%|          | 8/10672 [00:00<02:21, 75.44it/s]epoch: 8 loss: 1.667904 accuracy: 0.414543\n",
    "100%|██████████| 10672/10672 [02:12<00:00, 80.39it/s]epoch: 9 loss: 1.123815 accuracy: 0.413981\n",
    "Finished Training\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 83. ミニバッチ化・GPU上での学習\n",
    "問題82のコードを改変し，B事例ごとに損失・勾配を計算して学習を行えるようにせよ（Bの値は適当に選べ）．また，GPU上で学習を実行せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "0%|          | 0/11 [00:00<?, ?it/s]\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-342-3c4f417c9218>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mn_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNN(dw, dh, L, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "X_train = pad_sequence(X_train, batch_first=True)\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=1024, shuffle=True)\n",
    "\n",
    "model   = model.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    hidden = torch.zeros(1, 121, dh, dtype=torch.float32)\n",
    "    n_correct = 0\n",
    "    total_loss = 0\n",
    "    for inputs, label in tqdm(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        label = label.to(device)\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        hidden = hidden.detach()\n",
    "        loss = criterion(outputs, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() # パラメータを更新\n",
    "\n",
    "        total_loss += loss.data \n",
    "        outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "        label = label.data.numpy()\n",
    "        if outputs == label:\n",
    "            n_correct += 1\n",
    "        \n",
    "    print('epoch: %d loss: %f accuracy: %f' % (epoch, loss, n_correct/dataset_size))\n",
    "\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 84. 単語ベクトルの導入\n",
    "事前学習済みの単語ベクトル（例えば，Google Newsデータセット（約1,000億単語）での学習済み単語ベクトル）で単語埋め込みemb(x)を初期化し，学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "googlenews = KeyedVectors.load_word2vec_format(\n",
    "    '../../data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, output_size, vocab_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(dw, dh, nonlinearity='relu')\n",
    "        self.liner = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, data, last_hidden):           # data: (max_len, dw)\n",
    "        y, hidden = self.rnn(data, last_hidden)     # y: (max_len, dh), hidden: (max_len, dh)\n",
    "        y = y[:,-1,:]\n",
    "        y = self.liner(y)\n",
    "        y = torch.softmax(y, dim=1)\n",
    "        return y, hidden\n",
    "\n",
    "\n",
    "def tokens2vec(tokens):\n",
    "    vec = []]\n",
    "    for token in tokens:\n",
    "        if token in googlenews:\n",
    "            vec.append(googlenews[token])\n",
    "        else:\n",
    "            vec.append([0]*dw)\n",
    "    return vec\n",
    "            \n",
    "\n",
    "X_train = train.tokens.apply(tokens2vec)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "\n",
    "Y_train = torch.tensor(Y_train).long()\n",
    "\n",
    "\n",
    "dataset_size = len(train)\n",
    "max_len = train.tokens.apply(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNN(dw, dh, L, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=8, shuffle=True)\n",
    "\n",
    "model   = model.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    hidden = torch.zeros(1, 121, dh, dtype=torch.float32)\n",
    "    n_correct = 0\n",
    "    total_loss = 0\n",
    "    for inputs, label in tqdm(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(outputs, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() # パラメータを更新\n",
    "        hidden = hidden.detach()\n",
    "        \n",
    "        total_loss += loss.data \n",
    "        outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "        label = label.data.numpy()\n",
    "        if outputs == label:\n",
    "            n_correct += 1\n",
    "        \n",
    "    print('epoch: %d loss: %f accuracy: %f' % (epoch, loss, correct_count/dataset_size))\n",
    "\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 85. 双方向RNN・多層化\n",
    "順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし，モデルを学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BidirectionalRNN(nn.Module):\n",
    "    def __init__(self, data_size, hidden_size, output_size, vocab_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(dw, dh, nonlinearity='relu', bidirectional=True)\n",
    "        self.liner = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, data, last_hidden):           # data: (max_len, dw)\n",
    "        y, hidden = self.rnn(data, last_hidden)     # y: (max_len, dh), hidden: (max_len, dh)\n",
    "        y = y[:,-1,:]\n",
    "        y = self.liner(y)\n",
    "        y = torch.softmax(y, dim=1)\n",
    "        return y, hidden\n",
    "\n",
    "\n",
    "def tokens2vec(tokens):\n",
    "    vec = []]\n",
    "    for token in tokens:\n",
    "        if token in googlenews:\n",
    "            vec.append(googlenews[token])\n",
    "        else:\n",
    "            vec.append([0]*dw)\n",
    "    return vec\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BidirectionalRNN(dw, dh, L, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=8, shuffle=True)\n",
    "\n",
    "model   = model.to(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    hidden = torch.zeros(1, 121, dh, dtype=torch.float32)\n",
    "    n_correct = 0\n",
    "    total_loss = 0\n",
    "    for inputs, label in tqdm(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(outputs, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() # パラメータを更新\n",
    "        hidden = hidden.detach()\n",
    "        \n",
    "        total_loss += loss.data \n",
    "        outputs = np.argmax(outputs.data.numpy(), axis=1)\n",
    "        label = label.data.numpy()\n",
    "        if outputs == label:\n",
    "            n_correct += 1\n",
    "        \n",
    "    print('epoch: %d loss: %f accuracy: %f' % (epoch, loss, correct_count/dataset_size))\n",
    "\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 86. 畳み込みニューラルネットワーク (CNN)\n",
    "ID番号で表現された単語列x=(x1,x2,…,xT)がある．ただし，Tは単語列の長さ，xt∈ℝVは単語のID番号のone-hot表記である（Vは単語の総数である）．畳み込みニューラルネットワーク（CNN: Convolutional Neural Network）を用い，単語列xからカテゴリyを予測するモデルを実装せよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 87. 確率的勾配降下法によるCNNの学習Permalink\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題86で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 88. パラメータチューニング\n",
    "問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 89. 事前学習済み言語モデルからの転移学習\n",
    "事前学習済み言語モデル（例えばBERTなど）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}