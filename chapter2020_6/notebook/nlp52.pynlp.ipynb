{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "model = Doc2Vec(documents, vector_size=300, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "fname = \"doc2vec_model\"\n",
    "model.save(fname)\n",
    "model = Doc2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hagaakari/.pyenv/versions/3.8.0/lib/python3.8/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "columns = ('id',\n",
    "           'title',\n",
    "           'category',\n",
    "           'story')\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.feature.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "valid = pd.read_csv('../../data/NewsAggregatorDataset/valid.feature.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "test = pd.read_csv('../../data/NewsAggregatorDataset/test.feature.txt',\n",
    "                   names=columns, sep='\\t')\n",
    "\n",
    "def tokenize(doc):\n",
    "    doc = re.sub(r\"[',.]\", '', doc)  # 記号を削除\n",
    "    tokens = doc.split(' ')\n",
    "    tokens = [token.lower() for token in tokens]  # 小文字に統一\n",
    "    return tokens\n",
    "\n",
    "def preprocessor(tokens):\n",
    "    tokens = [token for token in tokens if token not in common_words]\n",
    "    return tokens\n",
    "    \n",
    "def get_vector(doc):\n",
    "    \n",
    "    vector = model.infer_vector(doc)\n",
    "    vector = np.multiply(vector, 10000)\n",
    "    return pd.Series(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "train['tokens'] = train.title.apply(tokenize)\n",
    "vocab = train['tokens'].tolist()\n",
    "vocab = sum(vocab, [])  # flat list\n",
    "counter = Counter(vocab)\n",
    "vocab = [\n",
    "    token\n",
    "    for token, freq in counter.most_common()\n",
    "    if 2 < freq < 300\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10672/10672 [00:32<00:00, 324.32it/s]\n",
      "100%|██████████| 10672/10672 [00:06<00:00, 1707.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "train['tokens'] = train.tokens.progress_apply(preprocessor)\n",
    "X_train = train.tokens.progress_apply(get_vector) # 説明変数\n",
    "Y_train = train['category'].map({'b': 0, 't': 1, 'e': 2, 'm': 3}) # クラスを定義\n",
    "lr = LogisticRegression() # ロジスティック回帰モデルのインスタンスを作成\n",
    "lr.fit(X_train, Y_train) # ロジスティック回帰モデルの重みを学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept =  [ 0.87050237 -0.53406917  0.801551   -1.1379842 ]\n"
     ]
    }
   ],
   "source": [
    "# print(\"coefficient = \", lr.coef_)\n",
    "print(\"intercept = \", lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1334/1334 [00:04<00:00, 316.38it/s]\n",
      "100%|██████████| 10672/10672 [00:06<00:00, 1768.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 0 0 0 2 0 2 0 0 0 0 0 0 0 0 2 0 2 0 0 0 2 0 2 0 0 0 2 0 0 0 2 0 2 0 2\n",
      " 2 0 2 2 2 0 2 0 0 0 2 0 0 0 0 2 2 2 0 0 0 0 0 2 0 2 2 2 0 0 0 0 2 2 0 2 2\n",
      " 2 0 2 2 0 0 0 2 0 0 2 0 0 2 2 0 2 2 2 2 0 0 2 0 0 0]\n",
      "['t', 'e', 'b', 't', 'e', 'e', 'm', 'b', 'b', 'e', 'b', 'm', 'b', 'e', 't', 'e', 'b', 't', 'e', 'b', 'm', 't', 'e', 't', 'e', 'e', 'b', 'e', 'e', 'e', 't', 't', 'b', 'e', 'b', 'e', 'e', 'b', 'b', 'm', 'e', 'e', 'b', 'b', 'b', 'e', 't', 'b', 'e', 'e', 'b', 'b', 'e', 't', 'e', 'e', 'b', 'b', 'b', 'b', 'b', 'b', 'e', 'b', 'b', 'e', 't', 'b', 't', 'b', 'e', 'b', 'e', 'b', 'e', 'e', 'e', 'e', 'e', 't', 'e', 'e', 'e', 'm', 'm', 'b', 'b', 'b', 'e', 'b', 'b', 'b', 'e', 'b', 'e', 'b', 't', 't', 't', 'e']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test['tokens'] = test.title.apply(tokenize)\n",
    "test['tokens'] = test.tokens.progress_apply(preprocessor)\n",
    "X_test = train.tokens.progress_apply(get_vector)\n",
    "Y_pred = lr.predict(X_test)\n",
    "print(Y_pred[:100])\n",
    "print(test['category'].head(100).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5762"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    m\n",
       "1    b\n",
       "2    e\n",
       "3    e\n",
       "4    b\n",
       "5    b\n",
       "6    b\n",
       "7    t\n",
       "8    e\n",
       "9    b\n",
       "Name: category, dtype: object"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['category'].head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
