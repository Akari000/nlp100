{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "model = Doc2Vec(documents, vector_size=300, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "fname = \"doc2vec_model\"\n",
    "model.save(fname)\n",
    "model = Doc2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hagaakari/.pyenv/versions/3.8.0/lib/python3.8/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "columns = ('id',\n",
    "           'title',\n",
    "           'category',\n",
    "           'story')\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.feature.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "valid = pd.read_csv('../../data/NewsAggregatorDataset/valid.feature.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "test = pd.read_csv('../../data/NewsAggregatorDataset/test.feature.txt',\n",
    "                   names=columns, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def tokenize(doc):\n",
    "    doc = re.sub(r\"[',.]\", '', doc)  # 記号を削除\n",
    "    tokens = doc.split(' ')\n",
    "    tokens = [token.lower() for token in tokens]  # 小文字に統一\n",
    "    return tokens\n",
    "\n",
    "def preprocessor(tokens):\n",
    "    tokens = [token for token in tokens if token in vocab]\n",
    "    return tokens\n",
    "    \n",
    "def doc2vec(doc):\n",
    "    vector = model.infer_vector(doc)\n",
    "    vector = np.multiply(vector, 10000)\n",
    "    return pd.Series(vector)\n",
    "\n",
    "def bag_of_words(doc):\n",
    "    vector = [0]*len(vocab)\n",
    "    for word in doc:\n",
    "        if word in vocab:\n",
    "            vector[vocab.index(word)] += 1\n",
    "    return pd.Series(vector)\n",
    "\n",
    "def accuracy(predict, y):\n",
    "    return (predict == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "train['tokens'] = train.title.apply(tokenize)\n",
    "vocab = train['tokens'].tolist()\n",
    "vocab = sum(vocab, [])  # flat list\n",
    "counter = Counter(vocab)\n",
    "vocab = [\n",
    "    token\n",
    "    for token, freq in counter.most_common()\n",
    "    if 2 < freq < 300\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10672/10672 [00:01<00:00, 6636.59it/s]\n",
      "100%|██████████| 10672/10672 [00:06<00:00, 1593.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "train['tokens'] = train.tokens.progress_apply(preprocessor)\n",
    "X_train = train.tokens.progress_apply(bag_of_words) # 説明変数\n",
    "Y_train = train['category'].map({'b': 0, 't': 1, 'e': 2, 'm': 3}) # クラスを定義\n",
    "lr = LogisticRegression(class_weight='balanced') # ロジスティック回帰モデルのインスタンスを作成\n",
    "lr.fit(X_train, Y_train) # ロジスティック回帰モデルの重みを学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept =  [ 0.35953993 -0.0887583   0.42194464 -0.69272628]\n"
     ]
    }
   ],
   "source": [
    "# print(\"coefficient = \", lr.coef_)\n",
    "print(\"intercept = \", lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0 2 2 0 2 1 1 2 2]\n",
      "[3, 0, 2, 2, 0, 0, 0, 1, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "#  訓練データで予測\n",
    "Y_pred = lr.predict(X_train)\n",
    "Y_train = train['category'].map({'b': 0, 't': 1, 'e': 2, 'm': 3})\n",
    "print(Y_pred[:10])\n",
    "print(Y_train.head(10).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練データ 0.325712143928036\n"
     ]
    }
   ],
   "source": [
    "print('訓練データ', accuracy(Y_pred, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1334/1334 [00:00<00:00, 2057.06it/s]\n",
      "100%|██████████| 1334/1334 [00:00<00:00, 1726.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict [1 2 2 2 2 0 1 0 0 0]\n",
      "correct answer [1, 2, 0, 1, 2, 2, 3, 0, 0, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 評価データで予測\n",
    "test['tokens'] = test.title.apply(tokenize)\n",
    "test['tokens'] = test.tokens.progress_apply(preprocessor)\n",
    "X_test = test.tokens.progress_apply(doc2vec)  # 入力\n",
    "Y_pred = lr.predict(X_test)  # 予測\n",
    "Y_test = test['category'].map({'b': 0, 't': 1, 'e': 2, 'm': 3})  # 正解\n",
    "print('predict', Y_pred[:10])\n",
    "print('correct answer', Y_test.head(10).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "評価データ 0.24287856071964017\n"
     ]
    }
   ],
   "source": [
    "print('評価データ', accuracy(Y_pred, Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
