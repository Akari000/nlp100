{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 51. 特徴量抽出\n",
    "学習データ，検証データ，評価データから特徴量を抽出し，それぞれtrain.feature.txt，valid.feature.txt，test.feature.txtというファイル名で保存せよ． なお，カテゴリ分類に有用そうな特徴量は各自で自由に設計せよ．記事の見出しを単語列に変換したものが最低限のベースラインとなるであろう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hagaakari/.pyenv/versions/3.8.0/lib/python3.8/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "columns = ('id',\n",
    "           'title',\n",
    "           'category',\n",
    "           'story')\n",
    "\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.feature.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "valid = pd.read_csv('../../data/NewsAggregatorDataset/valid.feature.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "test = pd.read_csv('../../data/NewsAggregatorDataset/test.feature.txt',\n",
    "                   names=columns, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(doc):\n",
    "    doc = re.sub(r\"[',.]\", '', doc)  # 記号を削除\n",
    "    tokens = doc.split(' ')\n",
    "    tokens = [token.lower() for token in tokens]  # 小文字に統一\n",
    "    return tokens\n",
    "\n",
    "def preprocessor(tokens):\n",
    "    tokens = [token for token in tokens if token in vocab]\n",
    "    return tokens\n",
    "\n",
    "def bag_of_words(doc):\n",
    "    vector = [0]*len(vocab)\n",
    "    for word in doc:\n",
    "        if word in vocab:\n",
    "            vector[vocab.index(word)] += 1\n",
    "    return pd.Series(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary\n",
    "from collections import Counter\n",
    "\n",
    "train['tokens'] = train.title.apply(tokenize)\n",
    "vocab = train['tokens'].tolist()\n",
    "vocab = sum(vocab, [])  # flat list\n",
    "counter = Counter(vocab)\n",
    "vocab = [\n",
    "    token\n",
    "    for token, freq in counter.most_common()\n",
    "    if 2 < freq < 300\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10672/10672 [00:05<00:00, 2129.69it/s]\n",
      "100%|██████████| 10672/10672 [00:20<00:00, 509.11it/s]\n",
      "100%|██████████| 1334/1334 [00:00<00:00, 1996.46it/s]\n",
      "100%|██████████| 1334/1334 [00:02<00:00, 479.35it/s]\n",
      "100%|██████████| 1334/1334 [00:00<00:00, 2097.34it/s]\n",
      "100%|██████████| 1334/1334 [00:02<00:00, 511.08it/s]\n"
     ]
    }
   ],
   "source": [
    "train['tokens'] = train.tokens.progress_apply(preprocessor)\n",
    "X_train = train.tokens.progress_apply(bag_of_words)\n",
    "\n",
    "test['tokens'] = test.title.apply(tokenize)\n",
    "test['tokens'] = test.tokens.progress_apply(preprocessor)\n",
    "X_test = test.tokens.progress_apply(bag_of_words)\n",
    "\n",
    "valid['tokens'] = valid.title.apply(tokenize)\n",
    "valid['tokens'] = valid.tokens.progress_apply(preprocessor)\n",
    "X_valid = valid.tokens.progress_apply(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 52. 学習\n",
    "51で構築した学習データを用いて，ロジスティック回帰モデルを学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10672/10672 [00:05<00:00, 1866.74it/s]\n",
      "100%|██████████| 10672/10672 [00:21<00:00, 501.62it/s]\n",
      "/Users/hagaakari/.pyenv/versions/3.8.0/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Y_train = train['category'].map({'b': 0, 't': 1, 'e': 2, 'm': 3}) # クラスを定義\n",
    "lr = LogisticRegression(class_weight='balanced') # ロジスティック回帰モデルのインスタンスを作成\n",
    "lr.fit(X_train, Y_train) # ロジスティック回帰モデルの重みを学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient =  [[-0.63990247 -0.55659616 -1.08942751 ... -0.09404083 -0.06126723\n",
      "  -0.081567  ]\n",
      " [-0.5264873  -0.46478526 -0.4030986  ... -0.05961833 -0.03921252\n",
      "   0.15460631]\n",
      " [ 1.65678062  1.40852714  1.13529375 ...  0.19064374  0.1266009\n",
      "   0.0486148 ]\n",
      " [-0.49039084 -0.38714573  0.35723237 ... -0.03698459 -0.02612115\n",
      "  -0.12165411]]\n",
      "intercept =  [ 0.35953993 -0.0887583   0.42194464 -0.69272628]\n"
     ]
    }
   ],
   "source": [
    "print(\"coefficient = \", lr.coef_) # 説明変数の係数\n",
    "print(\"intercept = \", lr.intercept_) # 切片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 53. 予測\n",
    "52で学習したロジスティック回帰モデルを用い，与えられた記事見出しからカテゴリとその予測確率を計算するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測ラベル [3 0 2 2 0 0 0 1 2 0]\n",
      "正解 [3, 0, 2, 2, 0, 0, 0, 1, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "#  訓練データで予測\n",
    " \n",
    "Y_pred = lr.predict(X_train)\n",
    "Y_true = train['category'].map({'b': 0, 't': 1, 'e': 2, 'm': 3})\n",
    "print('予測ラベル', Y_pred[:10])\n",
    "print('正解', Y_true.head(10).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1334/1334 [00:00<00:00, 1879.01it/s]\n",
      "100%|██████████| 1334/1334 [00:02<00:00, 456.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予測ラベル [1 2 0 1 2 2 3 0 0 2]\n",
      "正解 [1, 2, 0, 1, 2, 2, 3, 0, 0, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 評価データで予測\n",
    "\n",
    "Y_pred = lr.predict(X_test)\n",
    "Y_true = test['category'].map({'b': 0, 't': 1, 'e': 2, 'm': 3})\n",
    "print('予測ラベル', Y_pred[:10])\n",
    "print('正解', Y_true.head(10).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 54. 正解率の計測\n",
    "52で学習したロジスティック回帰モデルの正解率を，学習データおよび評価データ上で計測せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predict, y):\n",
    "    return (predict == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10672/10672 [00:22<00:00, 476.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解率 0.9845389805097451\n"
     ]
    }
   ],
   "source": [
    "#  訓練データの正解率\n",
    "\n",
    "X_train = train.tokens.progress_apply(bag_of_words)\n",
    "Y_pred = lr.predict(X_train)\n",
    "Y_true = train['category'].map({'b': 0, 't': 1, 'e': 2, 'm': 3})\n",
    "\n",
    "print('正解率', accuracy(Y_pred, Y_true))\n",
    "# doc2vecでは 0.3256184407796102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1334/1334 [00:02<00:00, 494.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解率 0.8958020989505248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#  評価データの正解率\n",
    "\n",
    "X_test = test.tokens.progress_apply(bag_of_words)\n",
    "Y_pred = lr.predict(X_test)\n",
    "Y_true = test['category'].map({'b': 0, 't': 1, 'e': 2, 'm': 3})\n",
    "print('正解率', accuracy(Y_pred, Y_test))\n",
    "# doc2vecでは   0.24287856071964017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 55. 混同行列の作成\n",
    "52で学習したロジスティック回帰モデルの混同行列（confusion matrix）を，学習データおよび評価データ上で作成せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    size = len(set(y_true))\n",
    "    result = np.array([0]*(size*size)).reshape((size,size)) # 配列の初期化\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        result[t][p] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[498  36  10   9]\n",
      " [ 21 124   4   4]\n",
      " [ 14  14 505   5]\n",
      " [  7   8   7  68]]\n"
     ]
    }
   ],
   "source": [
    "con_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "print(con_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 56. 適合率，再現率，F1スコアの計測Permalink\n",
    "52で学習したロジスティック回帰モデルの適合率，再現率，F1スコアを，評価データ上で計測せよ．カテゴリごとに適合率，再現率，F1スコアを求め，カテゴリごとの性能をマイクロ平均（micro-average）とマクロ平均（macro-average）で統合せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(con_matrix):\n",
    "    size = len(con_matrix)\n",
    "    results = []\n",
    "    for i in range(size):\n",
    "        result = con_matrix[i][i]/sum(con_matrix[i])\n",
    "        results.append(result)\n",
    "    return results\n",
    "    \n",
    "def recall(con_matrix):\n",
    "    size = len(con_matrix)\n",
    "    results = []\n",
    "    for i in range(size):\n",
    "        result = con_matrix[i][i]/sum([row[i] for row in con_matrix])\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "def f1(pre, rec):\n",
    "    size = len(con_matrix)\n",
    "    results = []\n",
    "    for i in range(size):\n",
    "        result = (2*rec[i]*pre[i])/(rec[i]+pre[i])\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "presition [0.9005424954792043, 0.8104575163398693, 0.9386617100371747, 0.7555555555555555]\n",
      "recall [0.9222222222222223, 0.6813186813186813, 0.9600760456273765, 0.7906976744186046]\n",
      "f1 [0.9112534309240622, 0.7402985074626866, 0.9492481203007519, 0.7727272727272727]\n"
     ]
    }
   ],
   "source": [
    "pre = precision(con_matrix)\n",
    "rec = recall(con_matrix)\n",
    "f1_score = f1(pre, rec)\n",
    "print('presition', pre)\n",
    "print('recall', rec)\n",
    "print('f1', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "適合率 マクロ平均 0.8513043193529509\n",
      "再現率 マクロ平均 0.8385786558967212\n",
      "F1 マクロ平均 0.8433818328536933\n"
     ]
    }
   ],
   "source": [
    "# マクロ平均\n",
    "print('適合率 マクロ平均', np.array(pre).mean())\n",
    "print('再現率 マクロ平均', np.array(rec).mean())\n",
    "print('F1 マクロ平均', np.array(f1_score).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# マクロ平均\n",
    "print('適合率 マクロ平均', np.array(pre).mean())\n",
    "print('再現率 マクロ平均', np.array(rec).mean())\n",
    "print('F1 マクロ平均', np.array(f1_score).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
