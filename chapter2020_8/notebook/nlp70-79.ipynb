{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 70. 単語ベクトルの和による特徴量\n",
    "問題50で構築した学習データ，検証データ，評価データを行列・ベクトルに変換したい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''３データを持ってくる\n",
    "tokenizeする\n",
    "単語の特徴量をとる\n",
    "平均をとる\n",
    "xiができる\n",
    "Xを作る\n",
    "Yを作る'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from gensim.models import KeyedVectors\n",
    "tqdm.pandas()\n",
    "googlenews = KeyedVectors.load_word2vec_format(\n",
    "    '../../data/GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 300  # 単語ベクトルの次元\n",
    "\n",
    "# TODO np.zerosを使う\n",
    "\n",
    "def preprocessor(doc):\n",
    "    doc = re.sub(r\"[',.]\", '', doc)     # 記号を削除\n",
    "    doc = doc.lower()             # 小文字に統一\n",
    "    return doc\n",
    "\n",
    "\n",
    "def tokenize(doc):\n",
    "    tokens = doc.split(' ')\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def emb(token):\n",
    "    if token in googlenews:\n",
    "        return googlenews[token]\n",
    "    else:\n",
    "        return [0.0]*d\n",
    "\n",
    "\n",
    "def get_x(tokens): # t = 0 の場合ここでnanが出てしまっていた．\n",
    "    t = len(tokens)\n",
    "    x = np.array([0.0]*d)\n",
    "    if t == 0:\n",
    "        return x\n",
    "    for token in tokens:\n",
    "        x += np.array(emb(token))\n",
    "    return x/t\n",
    "\n",
    "\n",
    "def reduce_vocab(tokens):\n",
    "    tokens = [token for token in tokens if token in googlenews]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def is_empty(tokens):\n",
    "    return len(tokens) == 0\n",
    "\n",
    "\n",
    "def bag_of_tokens(doc):\n",
    "    vector = [0]*len(vocab)\n",
    "    for token in doc:\n",
    "        if token in vocab:\n",
    "            vector[vocab.index(token)] += 1\n",
    "    return pd.Series(vector)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ('category', 'title')\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "valid = pd.read_csv('../../data/NewsAggregatorDataset/valid.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "test = pd.read_csv('../../data/NewsAggregatorDataset/test.txt',\n",
    "                   names=columns, sep='\\t')\n",
    "print(len(train))\n",
    "# TODO 書き方がくどくなってしまったので，もう少しまとめる\n",
    "# テストデータでもis emptyが出てくる可能性があるため．is emptyの時に省いてしまうのはダメ．零ベクトルとして扱うのが良い．\n",
    "\n",
    "# preprocess\n",
    "train['tokens'] = train.title.progress_apply(preprocessor)\n",
    "test['tokens'] = test.title.progress_apply(preprocessor)\n",
    "valid['tokens'] = valid.title.progress_apply(preprocessor)\n",
    "\n",
    "# tokenize\n",
    "train['tokens'] = train.tokens.apply(tokenize)\n",
    "test['tokens'] = test.tokens.apply(tokenize)\n",
    "valid['tokens'] = valid.tokens.apply(tokenize)\n",
    "\n",
    "# reduce vocablary\n",
    "train['tokens'] = train.tokens.apply(reduce_vocab)\n",
    "test['tokens'] = test.tokens.apply(reduce_vocab)\n",
    "valid['tokens'] = valid.tokens.apply(reduce_vocab)\n",
    "\n",
    "train['is_empty'] = train.tokens.apply(is_empty)\n",
    "test['is_empty'] = test.tokens.apply(is_empty)\n",
    "valid['is_empty'] = valid.tokens.apply(is_empty)\n",
    "\n",
    "train = train[train.is_empty == False]\n",
    "test = test[test.is_empty == False]\n",
    "valid = valid[valid.is_empty == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "X_train = np.array(train.tokens.apply(get_x).values.tolist())\n",
    "X_valid = np.array(valid.tokens.apply(get_x).values.tolist())\n",
    "X_test = np.array(test.tokens.apply(get_x).values.tolist())\n",
    "\n",
    "label2int = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "Y_train = np.array(train.category.map(label2int))\n",
    "Y_valid = np.array(valid.category.map(label2int))\n",
    "Y_test = np.array(test.category.map(label2int))\n",
    "\n",
    "np.save('../../data/nlp2020_70/X_train', X_train)\n",
    "np.save('../../data/nlp2020_70/X_valid', X_valid)\n",
    "np.save('../../data/nlp2020_70/X_test', X_test)\n",
    "np.save('../../data/nlp2020_70/Y_train', Y_train)\n",
    "np.save('../../data/nlp2020_70/Y_valid', Y_valid)\n",
    "np.save('../../data/nlp2020_70/Y_test', Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 71. 単層ニューラルネットワークによる予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "X_train = np.load('../../data/nlp2020_70/X_train.npy')\n",
    "X_valid = np.load('../../data/nlp2020_70/X_valid.npy')\n",
    "X_test = np.load('../../data/nlp2020_70/X_test.npy')\n",
    "Y_train = np.load('../../data/nlp2020_70/Y_train.npy')\n",
    "Y_valid = np.load('../../data/nlp2020_70/Y_valid.npy')\n",
    "Y_test = np.load('../../data/nlp2020_70/Y_test.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32) # 10672x300行列\n",
    "W = torch.randn(300, 4)  # 300x4行列\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "xW = torch.matmul(X_train[:1], W)\n",
    "XW = torch.matmul(X_train[:4], W)\n",
    "y = softmax(xW)[0]\n",
    "Y = softmax(XW)\n",
    "print(y)\n",
    "print(Y)\n",
    "'''\n",
    "\n",
    "tensor([[0.3985, 0.0194, 0.4485, 0.1337]])\n",
    "tensor([[0.3985, 0.0194, 0.4485, 0.1337],\n",
    "        [0.7893, 0.0462, 0.0289, 0.1356],\n",
    "        [0.8456, 0.0623, 0.0191, 0.0730],\n",
    "        [0.4550, 0.1102, 0.0192, 0.4156]])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.Moduleを使う場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(300, 4) # 重みを作成\n",
    "        nn.init.xavier_uniform_(self.fc.weight) # 一様分布の乱数で重みを初期化\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = torch.softmax(x, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntensor([0.2419, 0.2966, 0.2355, 0.2261], grad_fn=<SoftmaxBackward>)\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# なぜdim=-1でしかできない？\n",
    "# dim = -1 だと次元-1のところを見られる．\n",
    "# softmaxはNetの中に入れる\n",
    "\n",
    "model = Net()\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32) # 10672x300行列\n",
    "x = model(X_train[0])\n",
    "x\n",
    "\n",
    "'''\n",
    "tensor([0.2419, 0.2966, 0.2355, 0.2261], grad_fn=<SoftmaxBackward>)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntensor([[0.2419, 0.2966, 0.2355, 0.2261],\\n        [0.2209, 0.2652, 0.2306, 0.2833],\\n        [0.2366, 0.2597, 0.2268, 0.2769],\\n        [0.2327, 0.2713, 0.2357, 0.2604]], grad_fn=<SoftmaxBackward>)\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model(X_train[:4])\n",
    "x\n",
    "\n",
    "'''\n",
    "tensor([[0.2419, 0.2966, 0.2355, 0.2261],\n",
    "        [0.2209, 0.2652, 0.2306, 0.2833],\n",
    "        [0.2366, 0.2597, 0.2268, 0.2769],\n",
    "        [0.2327, 0.2713, 0.2357, 0.2604]], grad_fn=<SoftmaxBackward>)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 72. 損失と勾配の計算\n",
    "学習データの事例x1と事例集合x1,x2,x3,x4に対して，クロスエントロピー損失と，行列Wに対する勾配を計算せよ．なお，ある事例xiに対して損失は次式で計算される． \n",
    "\n",
    "    li = −log[事例xiがyiに分類される確率]  \n",
    "\n",
    "ただし，事例集合に対するクロスエントロピー損失は，その集合に含まれる各事例の損失の平均とする．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cross_entropy_loss(p):\n",
    "    return -1 * np.log(p)\n",
    "\n",
    "# xi\n",
    "x_loss = cross_entropy_loss(y[Y_train[0]])\n",
    "print('x loss: ', x_loss)\n",
    "\n",
    "#Xi\n",
    "X_loss = []\n",
    "for y, i  in zip(Y, Y_train[:4]):\n",
    "    X_loss.append(cross_entropy_loss(y[i]))\n",
    "\n",
    "# TODO 勾配を求める\n",
    "X_loss = np.mean(X_loss)\n",
    "print('X loss: ', X_loss)\n",
    "\n",
    "'''\n",
    "x loss:  tensor(1.9115)\n",
    "X loss:  1.4298041\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.Moduleを使う場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "損失 tensor(1.3930, grad_fn=<NllLossBackward>)\n",
      "勾配 tensor([[-1.1492e-03,  1.6141e-03, -4.0643e-03,  ..., -7.3275e-04,\n",
      "         -1.8659e-03, -2.9068e-04],\n",
      "        [ 1.5491e-03, -9.6670e-04,  1.5057e-03,  ..., -9.4094e-04,\n",
      "          6.9174e-04,  4.5292e-03],\n",
      "        [-2.1356e-03, -3.4372e-03,  5.7122e-03,  ...,  1.0617e-04,\n",
      "          1.2491e-03, -1.1650e-03],\n",
      "        [ 1.7357e-03,  2.7898e-03, -3.1536e-03,  ...,  1.5675e-03,\n",
      "         -7.4965e-05, -3.0735e-03]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n損失 tensor(1.3699, grad_fn=<NllLossBackward>)\\n勾配 tensor([[-0.0051,  0.0070, -0.0172,  ..., -0.0031, -0.0094, -0.0025],\\n        [ 0.0071, -0.0023,  0.0036,  ..., -0.0033,  0.0029,  0.0176],\\n        [-0.0080, -0.0136,  0.0235,  ...,  0.0014,  0.0058, -0.0064],\\n        [ 0.0060,  0.0088, -0.0099,  ...,  0.0050,  0.0006, -0.0087]])\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 勾配の求めかたがわからなかったのでtorch.nn.moduleを使って実装\n",
    "model = Net()\n",
    "criterion = torch.nn.CrossEntropyLoss() # クロスエントロピー損失関数を定義\n",
    "# クロスエントロピーを使う場合，この中にsoftmaxが含まれているので使う必要はない．\n",
    "かったのでtorch.nn.moduleを使って実装\n",
    "model = Net()\n",
    "criterion = torch.nn.CrossEntropyLoss() # クロスエントロピー損失関数を定義\n",
    "# クロスエントロピーを使う場合，この中にsoftmaxが含まれているので使う必要はない．\n",
    "# 使ってもそこまで影響はなさそう？\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32) # 10672x300行列\n",
    "Y_train = torch.tensor(Y_train).long() # long型にする\n",
    "\n",
    "inputs = X_train[:4]\n",
    "labels = Y_train[:4]\n",
    "\n",
    "outputs = model(inputs)\n",
    "loss = criterion(outputs, labels)\n",
    "model.zero_grad()  # 勾配をゼロにする．今回はループを回していないので無くて良い\n",
    "loss.backward()\n",
    "print('損失', loss)\n",
    "print('勾配', model.fc.weight.grad)\n",
    "'''\n",
    "損失 tensor(1.3699, grad_fn=<NllLossBackward>)\n",
    "勾配 tensor([[-0.0051,  0.0070, -0.0172,  ..., -0.0031, -0.0094, -0.0025],\n",
    "        [ 0.0071, -0.0023,  0.0036,  ..., -0.0033,  0.0029,  0.0176],\n",
    "        [-0.0080, -0.0136,  0.0235,  ...,  0.0014,  0.0058, -0.0064],\n",
    "        [ 0.0060,  0.0088, -0.0099,  ...,  0.0050,  0.0006, -0.0087]])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 73. 確率的勾配降下法による学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，行列Wを学習せよ．なお，学習は適当な基準で終了させればよい（例えば「100エポックで終了」など）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.load('../../data/nlp2020_70/X_train.npy')\n",
    "X_valid = np.load('../../data/nlp2020_70/X_valid.npy')\n",
    "X_test = np.load('../../data/nlp2020_70/X_test.npy')\n",
    "Y_train = np.load('../../data/nlp2020_70/Y_train.npy')\n",
    "Y_valid = np.load('../../data/nlp2020_70/Y_valid.npy')\n",
    "Y_test = np.load('../../data/nlp2020_70/Y_test.npy')\n",
    "\n",
    "# TODO ロスはepochごとに計算して合計をとる．\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32) #float32型にする\n",
    "Y_train = torch.tensor(Y_train).long() # long型にする\n",
    "\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=1, shuffle=True)\n",
    "\n",
    "prev_model = copy.deepcopy(model)\n",
    "prev_optimizer = copy.deepcopy(optimizer)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for inputs, labels in loader:\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() # パラメータを更新\n",
    "        \n",
    "        if torch.isnan(loss):\n",
    "            print(loss)\n",
    "            model = prev_model\n",
    "            optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "            optimizer.load_state_dict(prev_optimizer.state_dict())\n",
    "        else:\n",
    "            prev_model = copy.deepcopy(model)\n",
    "            prev_optimizer = copy.deepcopy(optimizer)\n",
    "            \n",
    "    print('epoch: %d loss: %f' % (epoch, loss))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "'''\n",
    "epoch: 0 loss: 0.606497\n",
    "epoch: 1 loss: 0.027537\n",
    "epoch: 2 loss: 0.718384\n",
    "epoch: 3 loss: 1.221328\n",
    "epoch: 4 loss: 0.476774\n",
    "epoch: 5 loss: 0.986584\n",
    "epoch: 6 loss: 0.080321\n",
    "epoch: 7 loss: 0.048740\n",
    "epoch: 8 loss: 0.053016\n",
    "epoch: 9 loss: 0.045396\n",
    "Finished Training\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 74. 正解率の計測\n",
    "問題73で求めた行列を用いて学習データおよび評価データの事例を分類したとき，その正解率をそれぞれ求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6851279167838066\n",
      "0.704647676161919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n0.8758316933745666\\n0.881559220389805\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(pred, label):\n",
    "    pred = np.argmax(pred.data.numpy(), axis=1) # 行ごとに最大値のインデックスを取得する．\n",
    "    label = label.data.numpy()\n",
    "    return (pred == label).mean()\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32) #float32型にする\n",
    "Y_test = torch.tensor(Y_test).long() # long型にする\n",
    "\n",
    "outputs = model(X_train)\n",
    "print (accuracy(outputs, Y_train))\n",
    "outputs = model(X_test)\n",
    "print (accuracy(outputs, Y_test))\n",
    "\n",
    "'''\n",
    "0.8758316933745666\n",
    "0.881559220389805\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 75. 損失と正解率のプロット\n",
    "問題73のコードを改変し，各エポックのパラメータ更新が完了するたびに，訓練データでの損失，正解率，検証データでの損失，正解率をグラフにプロットし，学習の進捗状況を確認できるようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-1ced8c4edf16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mloss_train_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0macc_train_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "# % matplotlib inline\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "X_train = np.load('../../data/nlp2020_70/X_train.npy')\n",
    "X_valid = np.load('../../data/nlp2020_70/X_valid.npy')\n",
    "X_test = np.load('../../data/nlp2020_70/X_test.npy')\n",
    "Y_train = np.load('../../data/nlp2020_70/Y_train.npy')\n",
    "Y_valid = np.load('../../data/nlp2020_70/Y_valid.npy')\n",
    "Y_test = np.load('../../data/nlp2020_70/Y_test.npy')\n",
    "\n",
    "# TODO trainを関数として切り出す．\n",
    "# TODO trainのロス，正解率はループの中で計算したものを使う．\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)  # 確率的勾配降下法\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32) #float32型にする\n",
    "Y_train = torch.tensor(Y_train).long() # long型にする\n",
    "X_valid = torch.tensor(X_valid, dtype=torch.float32) #float32型にする\n",
    "Y_valid = torch.tensor(Y_valid).long() # long型にする\n",
    "\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=1, shuffle=True)\n",
    "\n",
    "prev_model = copy.deepcopy(model)\n",
    "prev_optimizer = copy.deepcopy(optimizer)\n",
    "\n",
    "loss_train_list = []\n",
    "loss_valid_list = []\n",
    "acc_train_list = []\n",
    "acc_valid_list = []\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    for index, data in enumerate(loader):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            model = prev_model\n",
    "            optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "            optimizer.load_state_dict(prev_optimizer.state_dict())\n",
    "        else:\n",
    "            prev_model = copy.deepcopy(model)\n",
    "            prev_optimizer = copy.deepcopy(optimizer)\n",
    "\n",
    "    # train\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, Y_train)\n",
    "    acc = accuracy(outputs, Y_train)\n",
    "    loss_train_list.append(loss.data.numpy())\n",
    "    acc_train_list.append(acc)\n",
    "    print('train\\tepoch: %d loss: %f accuracy: %f' % (epoch, loss, acc))\n",
    "\n",
    "    # valid\n",
    "    outputs = model(X_valid)\n",
    "    loss = criterion(outputs, Y_valid)\n",
    "    acc = accuracy(outputs, Y_valid)\n",
    "    loss_valid_list.append(loss.data.numpy())\n",
    "    acc_valid_list.append(acc)\n",
    "    print('valid\\tepoch: %d loss: %f accuracy: %f' % (epoch, loss, acc))\n",
    "\n",
    "    # plot\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.plot(range(epoch+1), loss_train_list, label=\"train\")\n",
    "    plt.plot(range(epoch+1), loss_valid_list, label=\"valid\")\n",
    "    plt.legend()\n",
    "    plt.savefig('../nlp75_loss.png')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.plot(range(epoch+1), acc_train_list, label=\"train\")\n",
    "    plt.plot(range(epoch+1), acc_valid_list, label=\"valid\")\n",
    "    plt.legend()\n",
    "    plt.savefig('../nlp75_accuracy.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 76. チェックポイント\n",
    "問題75のコードを改変し，各エポックのパラメータ更新が完了するたびに，チェックポイント（学習途中のパラメータ（重み行列など）の値や最適化アルゴリズムの内部状態）をファイルに書き出せ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % matplotlib inline\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "X_train = np.load('../../data/nlp2020_70/X_train.npy')\n",
    "X_valid = np.load('../../data/nlp2020_70/X_valid.npy')\n",
    "X_test = np.load('../../data/nlp2020_70/X_test.npy')\n",
    "Y_train = np.load('../../data/nlp2020_70/Y_train.npy')\n",
    "Y_valid = np.load('../../data/nlp2020_70/Y_valid.npy')\n",
    "Y_test = np.load('../../data/nlp2020_70/Y_test.npy')\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)  # 確率的勾配降下法\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32) #float32型にする\n",
    "Y_train = torch.tensor(Y_train).long() # long型にする\n",
    "\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=1, shuffle=True)\n",
    "\n",
    "prev_model = copy.deepcopy(model)\n",
    "prev_optimizer = copy.deepcopy(optimizer)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for index, data in enumerate(loader):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            model = prev_model\n",
    "            optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "            optimizer.load_state_dict(prev_optimizer.state_dict())\n",
    "        else:\n",
    "            prev_model = copy.deepcopy(model)\n",
    "            prev_optimizer = copy.deepcopy(optimizer)\n",
    "\n",
    "    torch.save(model.state_dict(), '../nlp76_%d.model' % (epoch))\n",
    "    torch.save(optimizer.state_dict(), '../nlp76_%d.palams' % (epoch)) # 学習を再開する場合はoptimizerを保存しておく．両方保存しておくのが無難．\n",
    "    print('epoch: %d loss: %f accuracy: %f' % (epoch, loss, acc))\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../nlp76_0.model')\n",
    "model\n",
    "'''\n",
    "odict_items([\n",
    "('fc.weight', \n",
    "tensor([[-0.1008, -0.1395, -0.1014,  ..., -0.0492,  0.1527, -0.2046],\n",
    "        [ 0.1128, -0.1131,  0.0435,  ...,  0.0622,  0.0962, -0.0228],\n",
    "        [-0.1079,  0.1268, -0.0365,  ..., -0.0685, -0.1348,  0.0695],\n",
    "        [-0.0075,  0.0643,  0.0924,  ...,  0.1210,  0.0523,  0.0239]])), \n",
    "('fc.bias', tensor([ 0.4536, -0.3080,  0.4848, -0.6180]))])\n",
    "'''\n",
    "\n",
    "params = torch.load('../nlp76_0.palams')\n",
    "params\n",
    "'''\n",
    "{'state': {},\n",
    " 'param_groups': [{'lr': 0.001,\n",
    "   'momentum': 0,\n",
    "   'dampening': 0,\n",
    "   'weight_decay': 0,\n",
    "   'nesterov': False,\n",
    "   'params': [10752651200, 10752656896]}]}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 77. ミニバッチ化\n",
    "問題76のコードを改変し，B事例ごとに損失・勾配を計算し，行列Wの値を更新せよ（ミニバッチ化）．Bの値を1,2,4,8,…と変化させながら，1エポックの学習に要する時間を比較せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % matplotlib inline\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "X_train = np.load('../../data/nlp2020_70/X_train.npy')\n",
    "X_valid = np.load('../../data/nlp2020_70/X_valid.npy')\n",
    "X_test = np.load('../../data/nlp2020_70/X_test.npy')\n",
    "Y_train = np.load('../../data/nlp2020_70/Y_train.npy')\n",
    "Y_valid = np.load('../../data/nlp2020_70/Y_valid.npy')\n",
    "Y_test = np.load('../../data/nlp2020_70/Y_test.npy')\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)  # 確率的勾配降下法\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32) #float32型にする\n",
    "Y_train = torch.tensor(Y_train).long() # long型にする\n",
    "\n",
    "\n",
    "prev_model = copy.deepcopy(model)\n",
    "prev_optimizer = copy.deepcopy(optimizer)\n",
    "\n",
    "times = []\n",
    "\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "\n",
    "for bach in [2**x for x in range(10)]:\n",
    "    loader = DataLoader(ds, batch_size=bach, shuffle=True)\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(1):\n",
    "        for index, data in enumerate(loader):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                model = prev_model\n",
    "                optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "                optimizer.load_state_dict(prev_optimizer.state_dict())\n",
    "            else:\n",
    "                prev_model = copy.deepcopy(model)\n",
    "                prev_optimizer = copy.deepcopy(optimizer)\n",
    "\n",
    "    times.append(time.time() - start)\n",
    "    print('epoch: %d loss: %f accuracy: %f' % (epoch, loss, acc))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "times\n",
    "\n",
    "'''\n",
    "[5.0293779373168945,\n",
    " 2.6831369400024414,\n",
    " 1.2833211421966553,\n",
    " 0.7357909679412842]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 78. GPU上での学習\n",
    "問題77のコードを改変し，GPU上で学習を実行せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelとデータセットを to_deviseするだけ\n",
    "device = torch.device('cuda')\n",
    "model = Net()\n",
    "task = Task()\n",
    "loaders = (\n",
    "    gen_loader(train_dataset, 128, shuffle = True),\n",
    "    gen_loader(valid_dataset, 1),\n",
    ")\n",
    "optimizer = optim.SGD(model.parameters(), 0.1 * 128)\n",
    "trainer = Trainer(model, loaders, task, optimizer, 3, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 79. 多層ニューラルネットワーク\n",
    "問題78のコードを改変し，バイアス項の導入や多層化など，ニューラルネットワークの形状を変更しながら，高性能なカテゴリ分類器を構築せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(300, 128) # 重みを作成\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 4)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight) # 一様分布の乱数で重みを初期化\n",
    "        nn.init.xavier_uniform_(self.fc2.weight) # 一様分布の乱数で重みを初期化\n",
    "    \n",
    "    def forward(self, x): \n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % matplotlib inline\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "X_train = np.load('../../data/nlp2020_70/X_train.npy')\n",
    "X_valid = np.load('../../data/nlp2020_70/X_valid.npy')\n",
    "X_test = np.load('../../data/nlp2020_70/X_test.npy')\n",
    "Y_train = np.load('../../data/nlp2020_70/Y_train.npy')\n",
    "Y_valid = np.load('../../data/nlp2020_70/Y_valid.npy')\n",
    "Y_test = np.load('../../data/nlp2020_70/Y_test.npy')\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)  # 確率的勾配降下法\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32) #float32型にする\n",
    "Y_train = torch.tensor(Y_train).long() # long型にする\n",
    "X_valid = torch.tensor(X_valid, dtype=torch.float32) #float32型にする\n",
    "Y_valid = torch.tensor(Y_valid).long() # long型にする\n",
    "\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=8, shuffle=True)\n",
    "\n",
    "prev_model = copy.deepcopy(model)\n",
    "prev_optimizer = copy.deepcopy(optimizer)\n",
    "\n",
    "loss_train_list = []\n",
    "loss_valid_list = []\n",
    "acc_train_list = []\n",
    "acc_valid_list = []\n",
    "times = []\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(100):\n",
    "    for index, data in enumerate(loader):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            model = prev_model\n",
    "            optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "            optimizer.load_state_dict(prev_optimizer.state_dict())\n",
    "        else:\n",
    "            prev_model = copy.deepcopy(model)\n",
    "            prev_optimizer = copy.deepcopy(optimizer)\n",
    "\n",
    "    times.append(time.time() - start)\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.plot(range(epoch+1), loss_train_list, label=\"train\")\n",
    "    plt.plot(range(epoch+1), loss_valid_list, label=\"valid\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig('../nlp79_loss.png')\n",
    "\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.plot(range(epoch+1), acc_train_list, label=\"train\")\n",
    "    plt.plot(range(epoch+1), acc_valid_list, label=\"valid\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig('../nlp79_accuracy.png')\n",
    "\n",
    "        \n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (20,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-cb67105186bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_valid_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2759\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2760\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2761\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[1;32m   2763\u001b[0m         is not None else {}), **kwargs)\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \"\"\"\n\u001b[1;32m   1645\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1646\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1647\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (20,) and (0,)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(range(20), loss_train_list, label=\"train\")\n",
    "plt.plot(range(20), loss_valid_list, label=\"valid\")\n",
    "plt.legend()\n",
    "plt.savefig('../nlp79_loss.png')\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.plot(range(20), acc_train_list, label=\"train\")\n",
    "plt.plot(range(20), acc_valid_list, label=\"valid\")\n",
    "plt.legend()\n",
    "plt.savefig('../nlp79_accuracy.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
