{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 70. 単語ベクトルの和による特徴量\n",
    "問題50で構築した学習データ，検証データ，評価データを行列・ベクトルに変換したい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''３データを持ってくる\n",
    "tokenizeする\n",
    "単語の特徴量をとる\n",
    "平均をとる\n",
    "xiができる\n",
    "Xを作る\n",
    "Yを作る'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from gensim.models import KeyedVectors\n",
    "tqdm.pandas()\n",
    "googlenews = KeyedVectors.load_word2vec_format(\n",
    "    '../../data/GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 300  # 単語ベクトルの次元\n",
    "\n",
    "# TODO np.zerosを使う\n",
    "\n",
    "def preprocessor(doc):\n",
    "    doc = re.sub(r\"[',.]\", '', doc)     # 記号を削除\n",
    "    doc = doc.lower()             # 小文字に統一\n",
    "    return doc\n",
    "\n",
    "\n",
    "def tokenize(doc):\n",
    "    tokens = doc.split(' ')\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def emb(token):\n",
    "    if token in googlenews:\n",
    "        return googlenews[token]\n",
    "    else:\n",
    "        return [0.0]*d\n",
    "\n",
    "\n",
    "def get_x(tokens): # t = 0 の場合ここでnanが出てしまっていた．\n",
    "    t = len(tokens)\n",
    "    x = np.array([0.0]*d)\n",
    "    if t == 0:\n",
    "        return x\n",
    "    for token in tokens:\n",
    "        x += np.array(emb(token))\n",
    "    return x/t\n",
    "\n",
    "\n",
    "def reduce_vocab(tokens):\n",
    "    tokens = [token for token in tokens if token in googlenews]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def is_empty(tokens):\n",
    "    return len(tokens) == 0\n",
    "\n",
    "\n",
    "def bag_of_tokens(doc):\n",
    "    vector = [0]*len(vocab)\n",
    "    for token in doc:\n",
    "        if token in vocab:\n",
    "            vector[vocab.index(token)] += 1\n",
    "    return pd.Series(vector)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ('category', 'title')\n",
    "train = pd.read_csv('../../data/NewsAggregatorDataset/train.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "valid = pd.read_csv('../../data/NewsAggregatorDataset/valid.txt',\n",
    "                    names=columns, sep='\\t')\n",
    "test = pd.read_csv('../../data/NewsAggregatorDataset/test.txt',\n",
    "                   names=columns, sep='\\t')\n",
    "print(len(train))\n",
    "# preprocess\n",
    "train['tokens'] = train.title.progress_apply(preprocessor)\n",
    "test['tokens'] = test.title.progress_apply(preprocessor)\n",
    "valid['tokens'] = valid.title.progress_apply(preprocessor)\n",
    "\n",
    "# tokenize\n",
    "train['tokens'] = train.tokens.apply(tokenize)\n",
    "test['tokens'] = test.tokens.apply(tokenize)\n",
    "valid['tokens'] = valid.tokens.apply(tokenize)\n",
    "\n",
    "# reduce vocablary\n",
    "train['tokens'] = train.tokens.apply(reduce_vocab)\n",
    "test['tokens'] = test.tokens.apply(reduce_vocab)\n",
    "valid['tokens'] = valid.tokens.apply(reduce_vocab)\n",
    "\n",
    "train['is_empty'] = train.tokens.apply(is_empty)\n",
    "test['is_empty'] = test.tokens.apply(is_empty)\n",
    "valid['is_empty'] = valid.tokens.apply(is_empty)\n",
    "\n",
    "train = train[train.is_empty == False]\n",
    "test = test[test.is_empty == False]\n",
    "valid = valid[valid.is_empty == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "X_train = np.array(train.tokens.apply(get_x).values.tolist())\n",
    "X_valid = np.array(valid.tokens.apply(get_x).values.tolist())\n",
    "X_test = np.array(test.tokens.apply(get_x).values.tolist())\n",
    "\n",
    "label2int = {'b': 0, 't': 1, 'e': 2, 'm': 3}\n",
    "Y_train = np.array(train.category.map(label2int))\n",
    "Y_valid = np.array(valid.category.map(label2int))\n",
    "Y_test = np.array(test.category.map(label2int))\n",
    "\n",
    "np.save('../../data/nlp2020_70/X_train', X_train)\n",
    "np.save('../../data/nlp2020_70/X_valid', X_valid)\n",
    "np.save('../../data/nlp2020_70/X_test', X_test)\n",
    "np.save('../../data/nlp2020_70/Y_train', Y_train)\n",
    "np.save('../../data/nlp2020_70/Y_valid', Y_valid)\n",
    "np.save('../../data/nlp2020_70/Y_test', Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 71. 単層ニューラルネットワークによる予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "X_train = np.load('../../data/nlp2020_70/X_train.npy')\n",
    "X_valid = np.load('../../data/nlp2020_70/X_valid.npy')\n",
    "X_test = np.load('../../data/nlp2020_70/X_test.npy')\n",
    "Y_train = np.load('../../data/nlp2020_70/Y_train.npy')\n",
    "Y_valid = np.load('../../data/nlp2020_70/Y_valid.npy')\n",
    "Y_test = np.load('../../data/nlp2020_70/Y_test.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32) # 10672x300行列\n",
    "W = torch.randn(300, 4)  # 300x4行列\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "xW = torch.matmul(X_train[:1], W)\n",
    "XW = torch.matmul(X_train[:4], W)\n",
    "y = softmax(xW)[0]\n",
    "Y = softmax(XW)\n",
    "print(y)\n",
    "print(Y)\n",
    "'''\n",
    "\n",
    "tensor([[0.3985, 0.0194, 0.4485, 0.1337]])\n",
    "tensor([[0.3985, 0.0194, 0.4485, 0.1337],\n",
    "        [0.7893, 0.0462, 0.0289, 0.1356],\n",
    "        [0.8456, 0.0623, 0.0191, 0.0730],\n",
    "        [0.4550, 0.1102, 0.0192, 0.4156]])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.Moduleを使う場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(300, 4) # 重みを作成\n",
    "        nn.init.xavier_uniform_(self.fc.weight) # 一様分布の乱数で重みを初期化\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32) # 10672x300行列\n",
    "x = model(X_train[0])\n",
    "x = torch.softmax(x, dim=-1) # なぜdim=-1でしかできない？\n",
    "x\n",
    "'''\n",
    "tensor([0.2419, 0.2966, 0.2355, 0.2261], grad_fn=<SoftmaxBackward>)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model(X_train[:4])\n",
    "x = torch.softmax(x, dim=-1)\n",
    "x\n",
    "'''\n",
    "tensor([[0.2419, 0.2966, 0.2355, 0.2261],\n",
    "        [0.2209, 0.2652, 0.2306, 0.2833],\n",
    "        [0.2366, 0.2597, 0.2268, 0.2769],\n",
    "        [0.2327, 0.2713, 0.2357, 0.2604]], grad_fn=<SoftmaxBackward>)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 72. 損失と勾配の計算\n",
    "学習データの事例x1と事例集合x1,x2,x3,x4に対して，クロスエントロピー損失と，行列Wに対する勾配を計算せよ．なお，ある事例xiに対して損失は次式で計算される． \n",
    "\n",
    "    li = −log[事例xiがyiに分類される確率]  \n",
    "\n",
    "ただし，事例集合に対するクロスエントロピー損失は，その集合に含まれる各事例の損失の平均とする．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cross_entropy_loss(p):\n",
    "    return -1 * np.log(p)\n",
    "\n",
    "# xi\n",
    "x_loss = cross_entropy_loss(y[Y_train[0]])\n",
    "print('x loss: ', x_loss)\n",
    "\n",
    "#Xi\n",
    "X_loss = []\n",
    "for y, i  in zip(Y, Y_train[:4]):\n",
    "    X_loss.append(cross_entropy_loss(y[i]))\n",
    "\n",
    "# TODO 勾配を求める\n",
    "X_loss = np.mean(X_loss)\n",
    "print('X loss: ', X_loss)\n",
    "\n",
    "'''\n",
    "x loss:  tensor(1.9115)\n",
    "X loss:  1.4298041\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.Moduleを使う場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 勾配の求めかたがわからなかったのでtorch.nn.moduleを使って実装\n",
    "model = Net()\n",
    "criterion = torch.nn.CrossEntropyLoss() # クロスエントロピー損失関数を定義\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32) # 10672x300行列\n",
    "Y_train = torch.tensor(Y_train).long() # long型にする\n",
    "\n",
    "inputs = X_train[:4]\n",
    "labels = Y_train[:4]\n",
    "\n",
    "outputs = model(inputs)\n",
    "loss = criterion(outputs, labels)\n",
    "model.zero_grad()  # 勾配をゼロにする\n",
    "loss.backward()\n",
    "print('損失', loss)\n",
    "print('勾配', model.fc.weight.grad)\n",
    "'''\n",
    "損失 tensor(1.3699, grad_fn=<NllLossBackward>)\n",
    "勾配 tensor([[-0.0051,  0.0070, -0.0172,  ..., -0.0031, -0.0094, -0.0025],\n",
    "        [ 0.0071, -0.0023,  0.0036,  ..., -0.0033,  0.0029,  0.0176],\n",
    "        [-0.0080, -0.0136,  0.0235,  ...,  0.0014,  0.0058, -0.0064],\n",
    "        [ 0.0060,  0.0088, -0.0099,  ...,  0.0050,  0.0006, -0.0087]])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 73. 確率的勾配降下法による学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，行列Wを学習せよ．なお，学習は適当な基準で終了させればよい（例えば「100エポックで終了」など）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.load('../../data/nlp2020_70/X_train.npy')\n",
    "X_valid = np.load('../../data/nlp2020_70/X_valid.npy')\n",
    "X_test = np.load('../../data/nlp2020_70/X_test.npy')\n",
    "Y_train = np.load('../../data/nlp2020_70/Y_train.npy')\n",
    "Y_valid = np.load('../../data/nlp2020_70/Y_valid.npy')\n",
    "Y_test = np.load('../../data/nlp2020_70/Y_test.npy')\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # 確率的勾配降下法\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32) #float32型にする\n",
    "Y_train = torch.tensor(Y_train).long() # long型にする\n",
    "\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=1, shuffle=True)\n",
    "\n",
    "prev_model = copy.deepcopy(model)\n",
    "prev_optimizer = copy.deepcopy(optimizer)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for inputs, labels in loader:\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if torch.isnan(loss):\n",
    "            print(loss)\n",
    "            model = prev_model\n",
    "            optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "            optimizer.load_state_dict(prev_optimizer.state_dict())\n",
    "        else:\n",
    "            prev_model = copy.deepcopy(model)\n",
    "            prev_optimizer = copy.deepcopy(optimizer)\n",
    "            \n",
    "    print('epoch: %d loss: %f' % (epoch, loss))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "'''\n",
    "epoch: 0 loss: 0.606497\n",
    "epoch: 1 loss: 0.027537\n",
    "epoch: 2 loss: 0.718384\n",
    "epoch: 3 loss: 1.221328\n",
    "epoch: 4 loss: 0.476774\n",
    "epoch: 5 loss: 0.986584\n",
    "epoch: 6 loss: 0.080321\n",
    "epoch: 7 loss: 0.048740\n",
    "epoch: 8 loss: 0.053016\n",
    "epoch: 9 loss: 0.045396\n",
    "Finished Training\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 74. 正解率の計測\n",
    "問題73で求めた行列を用いて学習データおよび評価データの事例を分類したとき，その正解率をそれぞれ求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, label):\n",
    "    pred = np.argmax(pred.data.numpy(), axis=1) # 行ごとに最大値のインデックスを取得する．\n",
    "    label = label.data.numpy()\n",
    "    return (pred == label).mean()\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32) #float32型にする\n",
    "Y_test = torch.tensor(Y_test).long() # long型にする\n",
    "\n",
    "outputs = model(X_train)\n",
    "print (accuracy(outputs, Y_train))\n",
    "outputs = model(X_test)\n",
    "print (accuracy(outputs, Y_test))\n",
    "\n",
    "'''\n",
    "0.8758316933745666\n",
    "0.881559220389805\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 75. 損失と正解率のプロット\n",
    "問題73のコードを改変し，各エポックのパラメータ更新が完了するたびに，訓練データでの損失，正解率，検証データでの損失，正解率をグラフにプロットし，学習の進捗状況を確認できるようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# % matplotlib inline\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "X_train = np.load('../../data/nlp2020_70/X_train.npy')\n",
    "X_valid = np.load('../../data/nlp2020_70/X_valid.npy')\n",
    "X_test = np.load('../../data/nlp2020_70/X_test.npy')\n",
    "Y_train = np.load('../../data/nlp2020_70/Y_train.npy')\n",
    "Y_valid = np.load('../../data/nlp2020_70/Y_valid.npy')\n",
    "Y_test = np.load('../../data/nlp2020_70/Y_test.npy')\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)  # 確率的勾配降下法\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32) #float32型にする\n",
    "Y_train = torch.tensor(Y_train).long() # long型にする\n",
    "X_valid = torch.tensor(X_valid, dtype=torch.float32) #float32型にする\n",
    "Y_valid = torch.tensor(Y_valid).long() # long型にする\n",
    "\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=1, shuffle=True)\n",
    "\n",
    "prev_model = copy.deepcopy(model)\n",
    "prev_optimizer = copy.deepcopy(optimizer)\n",
    "\n",
    "loss_train_list = []\n",
    "loss_valid_list = []\n",
    "acc_train_list = []\n",
    "acc_valid_list = []\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    for index, data in enumerate(loader):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            model = prev_model\n",
    "            optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "            optimizer.load_state_dict(prev_optimizer.state_dict())\n",
    "        else:\n",
    "            prev_model = copy.deepcopy(model)\n",
    "            prev_optimizer = copy.deepcopy(optimizer)\n",
    "\n",
    "    # train\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, Y_train)\n",
    "    acc = accuracy(outputs, Y_train)\n",
    "    loss_train_list.append(loss.data.numpy())\n",
    "    acc_train_list.append(acc)\n",
    "    print('train\\tepoch: %d loss: %f accuracy: %f' % (epoch, loss, acc))\n",
    "\n",
    "    # valid\n",
    "    outputs = model(X_valid)\n",
    "    loss = criterion(outputs, Y_valid)\n",
    "    acc = accuracy(outputs, Y_valid)\n",
    "    loss_valid_list.append(loss.data.numpy())\n",
    "    acc_valid_list.append(acc)\n",
    "    print('valid\\tepoch: %d loss: %f accuracy: %f' % (epoch, loss, acc))\n",
    "\n",
    "    # plot\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.plot(range(epoch+1), loss_train_list, label=\"train\")\n",
    "    plt.plot(range(epoch+1), loss_valid_list, label=\"valid\")\n",
    "    plt.legend()\n",
    "    plt.savefig('../nlp75_loss.png')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.plot(range(epoch+1), acc_train_list, label=\"train\")\n",
    "    plt.plot(range(epoch+1), acc_valid_list, label=\"valid\")\n",
    "    plt.legend()\n",
    "    plt.savefig('../nlp75_accuracy.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 76. チェックポイント\n",
    "問題75のコードを改変し，各エポックのパラメータ更新が完了するたびに，チェックポイント（学習途中のパラメータ（重み行列など）の値や最適化アルゴリズムの内部状態）をファイルに書き出せ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % matplotlib inline\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "X_train = np.load('../../data/nlp2020_70/X_train.npy')\n",
    "X_valid = np.load('../../data/nlp2020_70/X_valid.npy')\n",
    "X_test = np.load('../../data/nlp2020_70/X_test.npy')\n",
    "Y_train = np.load('../../data/nlp2020_70/Y_train.npy')\n",
    "Y_valid = np.load('../../data/nlp2020_70/Y_valid.npy')\n",
    "Y_test = np.load('../../data/nlp2020_70/Y_test.npy')\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)  # 確率的勾配降下法\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32) #float32型にする\n",
    "Y_train = torch.tensor(Y_train).long() # long型にする\n",
    "\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=1, shuffle=True)\n",
    "\n",
    "prev_model = copy.deepcopy(model)\n",
    "prev_optimizer = copy.deepcopy(optimizer)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for index, data in enumerate(loader):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            model = prev_model\n",
    "            optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "            optimizer.load_state_dict(prev_optimizer.state_dict())\n",
    "        else:\n",
    "            prev_model = copy.deepcopy(model)\n",
    "            prev_optimizer = copy.deepcopy(optimizer)\n",
    "\n",
    "    torch.save(model.state_dict(), '../nlp76_%d.model' % (epoch))\n",
    "    torch.save(optimizer.state_dict(), '../nlp76_%d.palams' % (epoch))\n",
    "    print('epoch: %d loss: %f accuracy: %f' % (epoch, loss, acc))\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../nlp76_0.model')\n",
    "model\n",
    "'''\n",
    "odict_items([\n",
    "('fc.weight', \n",
    "tensor([[-0.1008, -0.1395, -0.1014,  ..., -0.0492,  0.1527, -0.2046],\n",
    "        [ 0.1128, -0.1131,  0.0435,  ...,  0.0622,  0.0962, -0.0228],\n",
    "        [-0.1079,  0.1268, -0.0365,  ..., -0.0685, -0.1348,  0.0695],\n",
    "        [-0.0075,  0.0643,  0.0924,  ...,  0.1210,  0.0523,  0.0239]])), \n",
    "('fc.bias', tensor([ 0.4536, -0.3080,  0.4848, -0.6180]))])\n",
    "'''\n",
    "\n",
    "params = torch.load('../nlp76_0.palams')\n",
    "params\n",
    "'''\n",
    "{'state': {},\n",
    " 'param_groups': [{'lr': 0.001,\n",
    "   'momentum': 0,\n",
    "   'dampening': 0,\n",
    "   'weight_decay': 0,\n",
    "   'nesterov': False,\n",
    "   'params': [10752651200, 10752656896]}]}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 77. ミニバッチ化\n",
    "問題76のコードを改変し，B事例ごとに損失・勾配を計算し，行列Wの値を更新せよ（ミニバッチ化）．Bの値を1,2,4,8,…と変化させながら，1エポックの学習に要する時間を比較せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % matplotlib inline\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "X_train = np.load('../../data/nlp2020_70/X_train.npy')\n",
    "X_valid = np.load('../../data/nlp2020_70/X_valid.npy')\n",
    "X_test = np.load('../../data/nlp2020_70/X_test.npy')\n",
    "Y_train = np.load('../../data/nlp2020_70/Y_train.npy')\n",
    "Y_valid = np.load('../../data/nlp2020_70/Y_valid.npy')\n",
    "Y_test = np.load('../../data/nlp2020_70/Y_test.npy')\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)  # 確率的勾配降下法\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32) #float32型にする\n",
    "Y_train = torch.tensor(Y_train).long() # long型にする\n",
    "\n",
    "\n",
    "prev_model = copy.deepcopy(model)\n",
    "prev_optimizer = copy.deepcopy(optimizer)\n",
    "\n",
    "times = []\n",
    "\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "\n",
    "for bach in [2**x for x in range(10)]:\n",
    "    loader = DataLoader(ds, batch_size=bach, shuffle=True)\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(1):\n",
    "        for index, data in enumerate(loader):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                model = prev_model\n",
    "                optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "                optimizer.load_state_dict(prev_optimizer.state_dict())\n",
    "            else:\n",
    "                prev_model = copy.deepcopy(model)\n",
    "                prev_optimizer = copy.deepcopy(optimizer)\n",
    "\n",
    "    times.append(time.time() - start)\n",
    "    print('epoch: %d loss: %f accuracy: %f' % (epoch, loss, acc))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "times\n",
    "\n",
    "'''\n",
    "[5.0293779373168945,\n",
    " 2.6831369400024414,\n",
    " 1.2833211421966553,\n",
    " 0.7357909679412842]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 78. GPU上での学習Permalink\n",
    "問題77のコードを改変し，GPU上で学習を実行せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 79. 多層ニューラルネットワーク\n",
    "問題78のコードを改変し，バイアス項の導入や多層化など，ニューラルネットワークの形状を変更しながら，高性能なカテゴリ分類器を構築せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(300, 128) # 重みを作成\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 4)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight) # 一様分布の乱数で重みを初期化\n",
    "        nn.init.xavier_uniform_(self.fc2.weight) # 一様分布の乱数で重みを初期化\n",
    "    \n",
    "    def forward(self, x): \n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % matplotlib inline\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "X_train = np.load('../../data/nlp2020_70/X_train.npy')\n",
    "X_valid = np.load('../../data/nlp2020_70/X_valid.npy')\n",
    "X_test = np.load('../../data/nlp2020_70/X_test.npy')\n",
    "Y_train = np.load('../../data/nlp2020_70/Y_train.npy')\n",
    "Y_valid = np.load('../../data/nlp2020_70/Y_valid.npy')\n",
    "Y_test = np.load('../../data/nlp2020_70/Y_test.npy')\n",
    "\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)  # 確率的勾配降下法\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32) #float32型にする\n",
    "Y_train = torch.tensor(Y_train).long() # long型にする\n",
    "X_valid = torch.tensor(X_valid, dtype=torch.float32) #float32型にする\n",
    "Y_valid = torch.tensor(Y_valid).long() # long型にする\n",
    "\n",
    "ds = TensorDataset(X_train, Y_train)\n",
    "loader = DataLoader(ds, batch_size=8, shuffle=True)\n",
    "\n",
    "prev_model = copy.deepcopy(model)\n",
    "prev_optimizer = copy.deepcopy(optimizer)\n",
    "\n",
    "loss_train_list = []\n",
    "loss_valid_list = []\n",
    "acc_train_list = []\n",
    "acc_valid_list = []\n",
    "times = []\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(100):\n",
    "    for index, data in enumerate(loader):\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            model = prev_model\n",
    "            optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "            optimizer.load_state_dict(prev_optimizer.state_dict())\n",
    "        else:\n",
    "            prev_model = copy.deepcopy(model)\n",
    "            prev_optimizer = copy.deepcopy(optimizer)\n",
    "\n",
    "    times.append(time.time() - start)ot\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.plot(range(epoch+1), loss_train_list, label=\"train\")\n",
    "    plt.plot(range(epoch+1), loss_valid_list, label=\"valid\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig('../nlp79_loss.png')\n",
    "\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.plot(range(epoch+1), acc_train_list, label=\"train\")\n",
    "    plt.plot(range(epoch+1), acc_valid_list, label=\"valid\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig('../nlp79_accuracy.png')\n",
    "\n",
    "        \n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(range(20), loss_train_list, label=\"train\")\n",
    "plt.plot(range(20), loss_valid_list, label=\"valid\")\n",
    "plt.legend()\n",
    "plt.savefig('../nlp79_loss.png')\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.plot(range(20), acc_train_list, label=\"train\")\n",
    "plt.plot(range(20), acc_valid_list, label=\"valid\")\n",
    "plt.legend()\n",
    "plt.savefig('../nlp79_accuracy.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}